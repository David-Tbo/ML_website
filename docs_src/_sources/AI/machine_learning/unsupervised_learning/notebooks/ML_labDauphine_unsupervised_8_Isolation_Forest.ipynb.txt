{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49be22f8",
   "metadata": {},
   "source": [
    "# **Clustering: Isolation Forest**  \n",
    "\n",
    "\n",
    "Manual Example on a Small Dataset\n",
    "\n",
    "**Goal**: Detect anomalies by isolating observations in a tree-based fashion.\n",
    "\n",
    "The idea is that anomalies are few and different, so the Isolation Forest algorithm isolates anomalies instead of profiling normal data points. It does this by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. The number of splits required to isolate a point is the anomaly score, with lower scores indicating anomalies.  \n",
    "\n",
    "Dataset (Augmented)\n",
    "\n",
    "| Point | Coordinates                     |\n",
    "| ----- | ------------------------------- |\n",
    "| A     | (1, 1)                          |\n",
    "| B     | (2, 1)                          |\n",
    "| C     | (4, 3)                          |\n",
    "| D     | (5, 4)                          |\n",
    "| E     | (3, 4)                          |\n",
    "| F     | (10, 10) ‚õ≥Ô∏è (potential anomaly) |\n",
    "\n",
    "* **Step 0: üí° Concept of Isolation Forest**  \n",
    "\n",
    "The Isolation Forest algorithm isolates observations by randomly selecting a feature and then randomly selecting a split value between the minimum and maximum of that feature.  \n",
    "\n",
    "Anomalies are more likely to be isolated faster (i.e., in fewer splits) because they are rare and distant from the majority of the data.  \n",
    "\n",
    "* **Step 1: üèóÔ∏è Build Isolation Trees (Intuitively)**  \n",
    "\n",
    "Let‚Äôs simulate how the Isolation Forest algorithm builds trees.  \n",
    "\n",
    "üîß Each tree is built by:  \n",
    "\n",
    "1. Randomly picking a feature (e.g., x or y),  \n",
    "2. Then randomly choosing a split value between the min and max of that feature,  \n",
    "3. Dividing the data into left/right (like in a decision tree),  \n",
    "4. Repeating the process recursively until every point is isolated in its own \"box\".   \n",
    "üí° An anomaly tends to be far away from other points, so it gets isolated faster, i.e., in fewer splits.  \n",
    "\n",
    "üß™ Example: Simulating One Tree  \n",
    "\n",
    "Let‚Äôs build one tree with a few random splits.  \n",
    "\n",
    "Initial Points:  \n",
    "\n",
    "A (1,1), B (2,1), C (4,3), D (5,4), E (3,4), F (10,10)  \n",
    "\n",
    "‚úÖ **Step-by-step simulation:**  \n",
    "\n",
    "1. Randomly choose x-axis, split at $x = 6$  \n",
    "    * All points with $x \\le 6 \\rightarrow$ left side  \n",
    "    * Point F $(x = 10)$ goes to the right $\\rightarrow$ F is immediately isolated!  \n",
    "        üü¢ Path length for $F = 1$  \n",
    "2. Now look at the left group: A, B, C, D, E  \n",
    "    Randomly choose y-axis, split at $y = 2$  \n",
    "    * Points A (1,1) and B (2,1) go left  \n",
    "    * Others go right  \n",
    "3. Let‚Äôs isolate A and B:  \n",
    "    Random split on x = 1.5 $\\rightarrow$  \n",
    "    * A (x=1) goes left $\\rightarrow$ A is isolated (Path length = 3)  \n",
    "    * B (x=2) goes right $\\rightarrow$ B is isolated (Path length = 3)  \n",
    "4. Same with C, D, E on the other side‚Ä¶.   \n",
    "    After several splits, they also get isolated, but it takes more steps.  \n",
    "\n",
    "This process is repeated many times (e.g., 100 trees), with different random splits each time.  \n",
    "For each point, we record how many splits were needed to isolate it in each tree.  \n",
    "\n",
    "Then, in **Step 2**, we compute the average path length for each point across all trees.  \n",
    "\n",
    "* **Step 2: üßÆ Average Path Lengths**  \n",
    "\n",
    "Let‚Äôs say after building many trees, we get:  \n",
    "\n",
    "| Point | Avg. Path Length           |\n",
    "| ----- | -------------------------- |\n",
    "| A     | 3.5                        |\n",
    "| B     | 3.4                        |\n",
    "| C     | 3.2                        |\n",
    "| D     | 3.3                        |\n",
    "| E     | 3.1                        |\n",
    "| F     | **1.2**‚õ≥Ô∏è (very few splits)|\n",
    "\n",
    "* **Step 3: üìâ Compute Anomaly Score**  \n",
    "\n",
    "Now we convert these path lengths into a score between 0 and 1 that tells us how \"anomalous\" a point is.  \n",
    "Here‚Äôs the formula:\n",
    "\n",
    "$$s(x,n) = 2^{-\\dfrac{-E(h(x))}{c(n)}}$$  \n",
    "\n",
    "Where:  \n",
    "* $E(h(x))$ is the average path length for point $x$ (from Step 2)  \n",
    "* $c(n) \\approx log(n) + 0.5772 - \\dfrac{1}{n}$ is the average path length in a random binary tree with $n$ points. We use this to normalize, so scores are comparable.      \n",
    "\n",
    "For our dataset with $6$ points:  \n",
    "$$c(6) \\approx log(6) + 0.5772 - \\dfrac{1}{6} \\approx 2.77$$  \n",
    "\n",
    "Now compute scores:  \n",
    " \n",
    "| Point | Path Length | Score                        | Interpretation       |\n",
    "| ----- | ----------- | ---------------------------- | -------------------- |\n",
    "| A     | 3.5         | $2^{-3.5/2.77} \\approx 0.29$ | Normal               |\n",
    "| B     | 3.4         | $\\approx 0.31$               | Normal               |\n",
    "| C     | 3.2         | $\\approx 0.35$               | Normal               |\n",
    "| D     | 3.3         | $\\approx 0.33$               | Normal               |\n",
    "| E     | 3.1         | $\\approx 0.36$               | Normal               |\n",
    "| F     | 1.2         | $2^{-1.2/2.77} \\approx 0.66$ | üö® Potential anomaly |\n",
    "\n",
    "‚úÖ Interpretation: The higher the score (closer to 1), the more likely the point is an anomaly. Threshold is often around 0.5.  \n",
    "\n",
    "‚úÖ So point F stands out as a potential anomaly!  \n",
    "\n",
    "* **Step 4: ‚úÖ Summary**  \n",
    "\n",
    "* Isolation Forest isolates outliers using random splits.\n",
    "* Outliers like F are isolated in fewer steps, leading to higher anomaly scores.\n",
    "* The score is based on average path length, normalized by the sample size.  \n",
    "\n",
    "‚úÖ Why Isolation Forest Works Well Here  \n",
    "\n",
    "* No need for distance or density estimates (unlike k-NN, LOF).\n",
    "* Scales well to large high-dimensional datasets.\n",
    "* Effective on small datasets with distinct outliers (like F = (10,10)).  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
