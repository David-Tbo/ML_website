{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression is a linear model where we try to estimate a probability instead of a specific value like in simple linear regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y_i$ ~ $B(p_i)$\n",
    "\n",
    "Les $Y_i$ suivent une loi de Bernoulli de paramètre $p_i$:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means that:\n",
    "\n",
    "$P(Y_i=1) = p_i$, $P(Y_i = 0) = 1 - p_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is equivalent to:  \n",
    "\n",
    "$P(Y_i = k) = {p_i}^k(1 - p_i)^{1-k}$ pour $k \\in \\{0, 1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample (_échantillon_)\n",
    "\n",
    "The joint distribution (_loi conjointe_) or joint probability of the $(Y_i)_{1, \\ldots, n}$ ~ $B(p_i)$ is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(Y_1=y_1, Y_2=y_2, \\ldots, Y_n=y_n) = \\underset{1,\\ldots, n}\\prod P(Y_i=y_i)$ , les $Y_i$ étant indépendants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $P(Y_i = y_i) = p_i^{y_i}(1-p_i)^{1-y_i}$  and considering the $Y_i$ independants and identically distributed $p_i = p$ , $ \\forall i \\in {1, \\ldots, n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\underset{1,\\ldots, n}\\prod P(Y_i=y_i) = p^{\\sum{y_i}}(1-p)^{\\sum{1-y_i}} = p^{\\sum{y_i}}(1-p)^{n - \\sum{y_i}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator of p : $\\hat{p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The likelihood (_vraissemblance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood is a function that measures the probability of observing a given sample.\n",
    "\n",
    "The likelihood is defined as the <u>joint probability</u> of the data given the model parameters $\\theta$:  \n",
    "\n",
    "$L_{\\theta}(Y_1,Y_2,\\ldots,Y_n) = \\underset{1,\\ldots, n}\\prod P_{\\theta}(Y_i=y_i)$.\n",
    "\n",
    "In our case, $\\theta = p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Maximum likelihood Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of the maximum likelihood method is to find the values of the parameter $\\theta$ that maximize the likelihood for the observed sample X.  \n",
    "In other words, we aim to find the values of the model parameters that make the observation of the given sample most probable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is where the likelihood reaches its maximum ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Thanks to its monotony, if we transform the likelihood into a log-likelihood, the maximum will be the same.  \n",
    "In other words, <u>find the maximum of the Likelihood is equivalent to find the maximum of the Log-likelihood</u>.\n",
    "\n",
    "A $log$ transformation will simplify the calculation:\n",
    "\n",
    "$\\log(L_{\\theta}(Y_1,Y_2,\\ldots,Y_n)) = \\log(\\underset{1,\\ldots, n}\\prod P_{\\theta}(Y_i=y_i)) = \\underset{1,\\ldots, n}\\sum \\log(P_{\\theta}(Y_i=y_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= \\underset{1,\\ldots, n}\\sum \\log (p^{y_i}(1-p)^{1-y_i}) = \\underset{1,\\ldots, n}\\sum [y_i\\log(p) + (1-y_i)\\log(1-p)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= \\underset{1,\\ldots, n}\\sum y_i\\log(p) + \\underset{1,\\ldots, n}\\sum(1-y_i)\\log(1-p) = \\underset{1,\\ldots, n}\\sum y_i\\log(p) + \\underset{1,\\ldots, n}\\sum\\log(1-p)  - \\underset{1,\\ldots, n}\\sum y_i\\log(1-p) = \\underset{1,\\ldots, n}\\sum y_i\\log(p) +  n\\log(1-p) - \\underset{1,\\ldots, n}\\sum y_i\\log(1-p)$\n",
    "\n",
    "$= \\underset{1,\\ldots, n}\\sum y_i\\log(p) - \\underset{1,\\ldots, n}\\sum y_i\\log(1-p) + n\\log(1-p) = \\underset{1,\\ldots, n}\\sum y_i\\log(\\frac{p}{1-p}) + n\\log(1-p)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now:  \n",
    "\n",
    "$\\log(L_{\\theta}(Y_1,Y_2,\\ldots,Y_n)) = \\underset{1,\\ldots, n}\\sum y_i\\log(\\frac{p}{1-p}) + n\\log(1-p)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Maximum Log-likelihood Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find where the Log-likelihood reaches its maximum we calculate the derivative of the Log-likelihood:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial }{\\partial \\theta} \\log(L_{\\theta}(Y_1,Y_2,\\ldots,Y_n)) =\\frac{\\partial }{\\partial \\theta} \\underset{1,\\ldots, n}\\sum y_i\\log(\\frac{p}{1-p}) + n\\log(1-p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta = p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial }{\\partial p} \\underset{1,\\ldots, n}\\sum y_i\\log(\\frac{p}{1-p}) + n\\log(1-p)= \\underset{1,\\ldots, n}\\sum  y_i \\frac{\\partial }{\\partial p} \\log(\\frac{p}{1-p}) + n \\frac{\\partial }{\\partial p} \\log(1-p) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- $ \\frac{\\partial }{\\partial p} \\log(\\frac{p}{1-p}) \\underset{(log(u))'=\\frac{u'}{u}}= \\frac{\\partial }{\\partial p} (\\frac{p}{1-p}) \\times \\frac{1-p}{p} = \\frac{1\\times(1-p)-p(-1)}{(1-p)^2} \\times \\frac{1-p}{p} = \\frac{1}{p(1-p)}$\n",
    "\n",
    "- $n \\frac{\\partial }{\\partial p} \\log(1-p) \\underset{(log(u))'=\\frac{u'}{u}}= n \\frac{(-1)}{1-p}$  \n",
    "\n",
    "$\\Rightarrow \\underset{1,\\ldots, n}\\sum  y_i \\frac{\\partial }{\\partial p} \\log(\\frac{p}{1-p}) + n \\frac{\\partial }{\\partial p} \\log(1-p) = \\underset{1,\\ldots, n}\\sum  y_i \\frac{1}{p(1-p)} + n  \\frac{(-1)}{1-p}  $  \n",
    "\n",
    "$\\Rightarrow \\frac{\\partial }{\\partial \\theta} \\log(L_{\\theta}(Y_1,Y_2,\\ldots,Y_n)) = 0$  \n",
    "\n",
    "$\\iff \\underset{1,\\ldots, n}\\sum  y_i \\frac{1}{p(1-p)} + n  \\frac{(-1)}{1-p} = 0$  \n",
    "\n",
    "$\\iff \\underset{1,\\ldots, n}\\sum  y_i \\frac{1}{p(1-p)}  = \\frac{n}{1-p}$  \n",
    "\n",
    "$\\iff \\frac{1}{n}\\underset{1,\\ldots, n}\\sum  y_i   = p$  \n",
    "\n",
    "$\\iff \\hat{p} = \\frac{1}{n}\\underset{1,\\ldots, n}\\sum  y_i = \\bar{y}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know now that the likelihood reaches a unique extremum with $\\bar{y}$\n",
    "\n",
    "Let's verfiy if this extremum is a maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, we would have compute: $\\frac{\\partial^2}{\\partial \\theta^2} \\log(L_{\\theta}(Y_1,Y_2,\\ldots,Y_n)) $ to study the **Fisher Information** $I(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$I(\\theta) = - \\mathbb{E} \\left[ \\frac{\\partial^2}{\\partial \\theta^2} \\log(L_{\\theta}(Y_1,Y_2,\\ldots,Y_n)) \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will rather study the sign of $\\frac{\\partial }{\\partial \\theta} \\log(L_{\\theta}(Y_1,Y_2,\\ldots,Y_n))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial }{\\partial \\theta} \\log(L_{\\theta}(Y_1,Y_2,\\ldots,Y_n)) > 0$  \n",
    "\n",
    "$\\iff \\underset{1,\\ldots, n}\\sum  y_i \\frac{1}{p(1-p)} + n  \\frac{(-1)}{1-p} > 0$  \n",
    "\n",
    "$\\iff \\underset{1,\\ldots, n}\\sum  y_i \\frac{1}{p(1-p)}  > \\frac{n}{1-p}$  \n",
    "\n",
    "$\\iff \\frac{1}{n}\\underset{1,\\ldots, n}\\sum  y_i   > p$  \n",
    "\n",
    "$\\iff \\bar{y} > p $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, $\\frac{\\partial }{\\partial \\theta} \\log(L_{\\theta}(Y_1,Y_2,\\ldots,Y_n)) < 0 \\iff \\bar{y} < p $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, If we construct a monotonicity table of the log-likelihood based on its partial derivative, $\\bar{y}$ is indeed a maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\bar{y}$ is unbiased\n",
    "\n",
    "$\\mathbb{E} \\left[ \\hat{p} \\right] = p \\underset{n \\infty} \\rightarrow p $ (évident)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Var} \\left[ \\hat{p} \\right] = \\text{Var} \\left[ \\frac{1}{n} \\sum Y_i \\right] = \\frac{1}{n^2} \\text{Var} \\left[  \\sum Y_i \\right] = \\frac{1}{n^2} \\sum \\text{Var} \\left[  Y_i \\right] = \\frac{1}{n^2} np(1-p) =  \\frac{1}{n} p(1-p) = \\frac{1}{n} I(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On atteint la borne de Cramer-Rao, notre variance est la plus faible de tous les estimateurs de p."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Var} \\left[ \\hat{p} \\right] = \\frac{p(1-p)}{n} \\underset{n \\infty} \\rightarrow 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow \\hat{p} $ est un Estimateur convergent Sans Biais de Variance Minimale (ESBVM). Il est parfait :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Rappel: La vraisemblance:\n",
    "\n",
    "Etant donnée un échantillon observé $(x_1, x_2,\\dots,x_n)$ et une loi de probabilité $P(\\theta)$, la vraisemblance quantifie la probabilité que les observations proviennent d'un échantillon théorique de loi $P(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple:  \n",
    "\n",
    "On effectue 10 lancers d'une pièce.  \n",
    "\n",
    "L'échantillon binaire observé est $0,1,1,0,1,1,1,0,0,1$\n",
    "\n",
    "Pour un échantillon de taille 10 de la loi de Bernoulli de paramètre $p_j=p$ la probabilité d'une telle réalisation est:  \n",
    "\n",
    "$p^6(1-p)^4$  \n",
    "\n",
    "| p | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 |\n",
    "|-----|---|---|---|---|---|---|---|\n",
    "| p^6(1-p)^4 | 2.6.10-5 | 1.8.10-4 | 5.3.10-4 | 9.8.10-4 | 1.2.10-3 | 9.5.10-4 | 4.2.10-4 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.62144e-05\n",
      "0.0001750329\n",
      "0.0005308416\n",
      "0.0009765625\n",
      "0.0011943936\n",
      "0.0009529569\n",
      "0.0004194304\n"
     ]
    }
   ],
   "source": [
    "for p in [.2, .3 , .4 , .5 , .6 , .7 , .8]:\n",
    "    likeli = round(p**6*(1-p)**4, 10)\n",
    "    print(likeli)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est naturel de choisir commme estimation de $p$ celle pour laquelle la probabilité de l'échantillon observé est la plus forte à savoir $p=0.6$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
