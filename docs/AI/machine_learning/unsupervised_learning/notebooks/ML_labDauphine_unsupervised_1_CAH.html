

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Clustering: HAC &mdash; Machine Learning and Deep Learning 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/nbsphinx-code-cells.css?v=2aa19091" />

  
      <script src="../../../../_static/documentation_options.js?v=2709fde1"></script>
      <script src="../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Clustering: KMeans" href="ML_labDauphine_unsupervised_2_Kmeans.html" />
    <link rel="prev" title="Unsupervised learning" href="../index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            Machine Learning and Deep Learning
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../index.html">Machine Learning</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../index.html#notebooks">Notebooks</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../index.html">Unsupervised learning</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="../index.html#notebooks">Notebooks</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">Machine Learning and Deep Learning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Machine Learning</a></li>
          <li class="breadcrumb-item"><a href="../index.html">Unsupervised learning</a></li>
      <li class="breadcrumb-item active">Clustering: HAC</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../_sources/AI/machine_learning/unsupervised_learning/notebooks/ML_labDauphine_unsupervised_1_CAH.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Clustering:-HAC">
<h1>Clustering: HAC<a class="headerlink" href="#Clustering:-HAC" title="Link to this heading">¬∂</a></h1>
<p>Hierarchical Agglomerative Clustering (HAC) / Classification Ascendante Hi√©rarchique (CAH)</p>
<p><strong>Presentation of the algorithm used in this notebook is Hierarchical Clustering (CAH) on a small dataset.</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># The packages

import numpy as np
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import linkage, dendrogram
</pre></div>
</div>
</div>
<section id="Reminder:-The-Euclidean-distance">
<h2>Reminder: The Euclidean distance<a class="headerlink" href="#Reminder:-The-Euclidean-distance" title="Link to this heading">¬∂</a></h2>
<p>In <strong>2D</strong> the <strong>Euclidean distance</strong> between two points <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> of respective coordinates <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> and <span class="math notranslate nohighlight">\((x_j, y_j)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{distance}_{euclidean}(i,j) = \sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}\]</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>a = np.array([8, 5])
b = np.array([2, 7])

print(np.sqrt(np.sum((a - b)**2)))

# Verification:
((8 - 2)**2 + (5 - 7)**2)**0.5==np.sqrt(np.sum((a - b)**2))
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
6.324555320336759
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
np.True_
</pre></div></div>
</div>
<p>We need to introduce <strong>For loop</strong> to calculate the euclidean distance of all the points of the clusters.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>a = np.array([8, 5])
b = np.array([2, 7])
c = np.array([3, 6])

cluster_1 = np.array([a, b])
print(f&quot;cluster_1: \n{cluster_1}&quot;)

cluster_2 = np.array([c])
print(f&quot;cluster_2: \n{cluster_2}&quot;)

for i in cluster_1:
    for j in cluster_2:
        print(f&quot;Distance between {i} and {j}: {np.sqrt(np.sum((i - j)**2))}&quot;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
cluster_1:
[[8 5]
 [2 7]]
cluster_2:
[[3 6]]
Distance between [8 5] and [3 6]: 5.0990195135927845
Distance between [2 7] and [3 6]: 1.4142135623730951
</pre></div></div>
</div>
<p>The <strong>single linkage cluster distance</strong> between two clusters <span class="math notranslate nohighlight">\(cluster_1\)</span> and <span class="math notranslate nohighlight">\(cluster_2\)</span> in <strong>2D</strong> is definied as:</p>
<div class="line-block">
<div class="line"><span class="math notranslate nohighlight">\(\text{for i cluster}_1\)</span></div>
<div class="line"><span class="math notranslate nohighlight">\(\text{for j in cluster}_2\)</span></div>
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="math notranslate nohighlight">
\[min(\text{distance}_{euclidean}(i,j) = \sqrt{(x_i - x_j)^2 + (y_i - y_j)^2})\]</div>
</div></blockquote>
<p><strong>Find the minimum distance between two clusters.</strong></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def cluster_distance(c1, c2):
    &#39;&#39;&#39;Using the euclidean distance to calculate the distance between two clusters.&#39;&#39;&#39;
    min_dist = float(&#39;inf&#39;)
    for i in c1:
        for j in c2:
            dx = i[0] - j[0]
            dy = i[1] - j[1]
            dist = (dx ** 2 + dy ** 2) ** 0.5
            if dist &lt; min_dist:
                min_dist = dist
    return min_dist


cluster_distance(cluster_1, cluster_2)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
np.float64(1.4142135623730951)
</pre></div></div>
</div>
</section>
<section id="Single-linkage-clustering---Step-by-step-Explanation">
<h2>Single linkage clustering - Step-by-step Explanation<a class="headerlink" href="#Single-linkage-clustering---Step-by-step-Explanation" title="Link to this heading">¬∂</a></h2>
<p>We will focus on <strong>single linkage</strong> (<em>saut minimum</em>) explanation based on proximity criteria:</p>
<p><span class="math notranslate nohighlight">\(\delta_{SL}(C_k, C_l) = \min\limits_{\substack{x_i \in C_k \\ x_j \in C_l}} \delta(x_i, x_j)\)</span></p>
<div class="line-block">
<div class="line">The explanations on <strong>complete linkage</strong> (<em>diam√®tre</em>) and <strong>average linkage</strong> (<em>moyenne</em>) are similar.</div>
<div class="line">Only the proximity criteria changes as follow respectively:</div>
</div>
<p><span class="math notranslate nohighlight">\(\delta_{SL}(C_k, C_l) = \max\limits_{\substack{x_i \in C_k \\ x_j \in C_l}} \delta(x_i, x_j)\)</span></p>
<p><span class="math notranslate nohighlight">\(\delta_{AL}(C_k, C_l) = \dfrac{1}{|C_k|\times|C_l|}\sum\limits_{\substack{x\in C_k, x_j \in C_l}}\delta(x_i, x_j)\)</span>.</p>
<p>We will use a dataset of 5 points with 2D coordinates to explain the HAC single linkage step-by-step. | Five 2D points | Coordinates | | ‚Äî‚Äì | ‚Äî‚Äî‚Äî‚Äì | | A | (1, 1) | | B | (2, 1) | | C | (4, 3) | | D | (5, 4) | | E | (3, 4) |</p>
<ul class="simple">
<li><p><strong>Step 1: Initialization</strong></p></li>
</ul>
<p>Each point is a cluster :</p>
<div class="line-block">
<div class="line">C1 = {A}</div>
<div class="line">C2 = {B}</div>
<div class="line">C3 = {C}</div>
<div class="line">C4 = {D}</div>
<div class="line">C5 = {E}</div>
</div>
<ul class="simple">
<li><p><strong>Step 2: distances calculation</strong></p></li>
</ul>
<p>The matrix of Euclidean distances</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>{A}</p></th>
<th class="head"><p>{B}</p></th>
<th class="head"><p>{C}</p></th>
<th class="head"><p>{D}</p></th>
<th class="head"><p>{E}</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>{A}</p></td>
<td><p>0</p></td>
<td><p>1.0</p></td>
<td><p>3.6</p></td>
<td><p>5.0</p></td>
<td><p>3.6</p></td>
</tr>
<tr class="row-odd"><td><p>{B}</p></td>
<td><p>1.0</p></td>
<td><p>0</p></td>
<td><p>2.8</p></td>
<td><p>4.2</p></td>
<td><p>3.2</p></td>
</tr>
<tr class="row-even"><td><p>{C}</p></td>
<td><p>3.6</p></td>
<td><p>2.8</p></td>
<td><p>0</p></td>
<td><p>1.4</p></td>
<td><p>1.4</p></td>
</tr>
<tr class="row-odd"><td><p>{D}</p></td>
<td><p>5.0</p></td>
<td><p>4.2</p></td>
<td><p>1.4</p></td>
<td><p>0</p></td>
<td><p>2.0</p></td>
</tr>
<tr class="row-even"><td><p>{E}</p></td>
<td><p>3.6</p></td>
<td><p>3.2</p></td>
<td><p>1.4</p></td>
<td><p>2.0</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p><strong>Step 3 Merge with single linkage</strong></p></li>
</ul>
<p>Minimale distance :</p>
<p>=&gt; A ‚Äì B = 1.0</p>
<p>Merge : C1 + C2 ‚Üí C1‚Äô = {A, B}</p>
<p>New clusters :</p>
<div class="line-block">
<div class="line">C1‚Äô = {A, B}</div>
<div class="line">C3 = {C}</div>
<div class="line">C4 = {D}</div>
<div class="line">C5 = {E}</div>
</div>
<ul class="simple">
<li><p><strong>Step 4: update the distances and merge</strong></p></li>
</ul>
<p>We update the distances:</p>
<div class="line-block">
<div class="line">{AB} ‚Äì {C} :</div>
<div class="line">= min(dist(A‚ÄìC), dist(B‚ÄìC))</div>
<div class="line">= min(3.6, 2.8)</div>
<div class="line">= 2.8</div>
</div>
<div class="line-block">
<div class="line">{AB} ‚Äì {D} :</div>
<div class="line">= min(dist(A‚ÄìD), dist(B‚ÄìD))</div>
<div class="line">= min(5.0, 4.2)</div>
<div class="line">= 4.2</div>
</div>
<div class="line-block">
<div class="line">{AB} ‚Äì {E} :</div>
<div class="line">= min(dist(A‚ÄìE), dist(B‚ÄìE))</div>
<div class="line">= min(3.6, 3.2)</div>
<div class="line">= 3.2</div>
</div>
<p>The matrix of Euclidean distances</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>{AB}</p></th>
<th class="head"><p>{C}</p></th>
<th class="head"><p>{D}</p></th>
<th class="head"><p>{E}</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>{AB}</p></td>
<td><p>0</p></td>
<td><p>2.8</p></td>
<td><p>4.2</p></td>
<td><p>3.2</p></td>
</tr>
<tr class="row-odd"><td><p>{C}</p></td>
<td><p>2.8</p></td>
<td><p>0</p></td>
<td><p>1.4</p></td>
<td><p>1.4</p></td>
</tr>
<tr class="row-even"><td><p>{D}</p></td>
<td><p>4.2</p></td>
<td><p>1.4</p></td>
<td><p>0</p></td>
<td><p>2.0</p></td>
</tr>
<tr class="row-odd"><td><p>{E}</p></td>
<td><p>3.2</p></td>
<td><p>1.4</p></td>
<td><p>2.0</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line">Two choices for the merge are possible: {C}+{D} or {C}+{E} as they both have the minimum distance = 1.4.</div>
<div class="line">We randomly choose to merge {C}+{D}.</div>
<div class="line">We have 3 clusters {AB}, {CD} and {E}.</div>
</div>
<p>We update the distances:</p>
<div class="line-block">
<div class="line">{CD} ‚Äì {AB} :</div>
<div class="line">= min(dist(C‚ÄìAB), dist(D‚ÄìAB))</div>
<div class="line">= min(2.8, 4.2)</div>
<div class="line">= 2.8</div>
</div>
<div class="line-block">
<div class="line">{CD} ‚Äì {E} :</div>
<div class="line">= min(dist(C‚ÄìE), dist(D‚ÄìE))</div>
<div class="line">= min(1.4, 2.0)</div>
<div class="line">= 1.4</div>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>{AB}</p></th>
<th class="head"><p>{CD}</p></th>
<th class="head"><p>{E}</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>{AB}</p></td>
<td><p>0</p></td>
<td><p>2.8</p></td>
<td><p>3.2</p></td>
</tr>
<tr class="row-odd"><td><p>{CD}</p></td>
<td><p>2.8</p></td>
<td><p>0</p></td>
<td><p>1.4</p></td>
</tr>
<tr class="row-even"><td><p>{E}</p></td>
<td><p>3.2</p></td>
<td><p>1.4</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line">We merge {CD} and {E}.</div>
<div class="line">We have now two clusters: {AB} and {CDE}.</div>
</div>
<p>We update the distances:</p>
<div class="line-block">
<div class="line">{CDE} ‚Äì {AB} :</div>
<div class="line">min(dist(A‚ÄìC), A‚ÄìD, A‚ÄìE, B‚ÄìC, B‚ÄìD, B‚ÄìE)</div>
<div class="line">= min(3.6, 5.0, 3.6, 2.8, 4.2, 3.2)</div>
<div class="line">= 2.8</div>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>{AB}</p></th>
<th class="head"><p>{CDE}</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>{AB}</p></td>
<td><p>0</p></td>
<td><p>2.8</p></td>
</tr>
<tr class="row-odd"><td><p>{CDE}</p></td>
<td><p>2.8</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p>We merge {AB} and {CDE} at 2.8 of distance from each other. üìâ Summary of the merges</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Step</p></th>
<th class="head"><p>Merge</p></th>
<th class="head"><p>Distance</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>A + B</p></td>
<td><p>1.0</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>C + D</p></td>
<td><p>1.4</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>{CD}+ E</p></td>
<td><p>1.4</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>{AB} + {CDE}</p></td>
<td><p>2.8</p></td>
</tr>
</tbody>
</table>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def print_simple_dendrogram():
    print(&quot;A   B        C   D   E&quot;)
    print(&quot;|   |        |   |   |&quot;)
    print(&quot; \\ /         \\ /    |      &lt;- 1.0 et 1.4&quot;)
    print(&quot;  AB          CD     |&quot;)
    print(&quot;          _____|_____/      &lt;- 1.4&quot;)
    print(&quot;         |           |&quot;)
    print(&quot;         |          CDE&quot;)
    print(&quot;         \\___________/      &lt;- 2.8 (final merge)&quot;)

print_simple_dendrogram()
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
A   B        C   D   E
|   |        |   |   |
 \ /         \ /    |      &lt;- 1.0 et 1.4
  AB          CD     |
          _____|_____/      &lt;- 1.4
         |           |
         |          CDE
         \___________/      &lt;- 2.8 (final merge)
</pre></div></div>
</div>
</section>
<section id="The-complexity">
<h2>The complexity<a class="headerlink" href="#The-complexity" title="Link to this heading">¬∂</a></h2>
<div class="line-block">
<div class="line">At first, the calculation of the matrix of distances which represent <span class="math notranslate nohighlight">\(n \times n = n^2\)</span> calculations</div>
<div class="line">But we don‚Äôt need to calculate the diagonal so we can substract of it <span class="math notranslate nohighlight">\(n\)</span> calculations.</div>
<div class="line">As we also only need to calculate the top of the matrix (matrix) to deduct the bottom, we can divide the calculations number by 2.</div>
</div>
<p>As a result, at initialization, the number of distances calculation is: <span class="math notranslate nohighlight">\((n^2 -n)/2 = n(n-1)/2\)</span>.</p>
<div class="line-block">
<div class="line">After the first merge we have <span class="math notranslate nohighlight">\(n-1\)</span> clusters so we will calculate <span class="math notranslate nohighlight">\((n-1)(n-2)/2\)</span> distances (we have use the previous formula).</div>
<div class="line">And <span class="math notranslate nohighlight">\((n-2)(n-3)/2\)</span> at the next step and so on ‚Ä¶</div>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Step (merge)</p></th>
<th class="head"><p>Remaining clusters</p></th>
<th class="head"><p>Distances comparisions</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0 (initial)</p></td>
<td><p>n clusters</p></td>
<td><p>n(n‚àí1)/2</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>n‚àí1 clusters</p></td>
<td><p>(n‚àí1)(n‚àí2)/2</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>n‚àí2 clusters</p></td>
<td><p>(n‚àí2)(n‚àí3)/2</p></td>
</tr>
<tr class="row-odd"><td><p>‚Ä¶</p></td>
<td><p>‚Ä¶</p></td>
<td><p>‚Ä¶</p></td>
</tr>
<tr class="row-even"><td><p>n‚àí2</p></td>
<td><p>2 clusters</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
<p>The total sum of the costs is:</p>
<p><span class="math notranslate nohighlight">\(\sum_{k=2}^{n}\dfrac{k(k-1)}{2} = \sum_{k=2}^{n}O(k^2) \approx{O(n^3)}\)</span></p>
<p>The naive HAC algorithm has cubic time complexity <span class="math notranslate nohighlight">\((O(n^3))\)</span>, due to repeated distance calculations at each step.</p>
<div class="line-block">
<div class="line">In contrast, optimized algorithms (e.g., SLINK for single linkage) use data structures that reuse previous computations,</div>
<div class="line">reducing the time complexity to <span class="math notranslate nohighlight">\(O(n^2)\)</span> or <span class="math notranslate nohighlight">\(O(n^2 log n)\)</span> depending on the linkage method.</div>
</div>
</section>
<section id="‚ö†Ô∏è-Limitations-in-High-Dimensional-Spaces">
<h2>‚ö†Ô∏è Limitations in High-Dimensional Spaces<a class="headerlink" href="#‚ö†Ô∏è-Limitations-in-High-Dimensional-Spaces" title="Link to this heading">¬∂</a></h2>
<div class="line-block">
<div class="line">Hierarchical Agglomerative Clustering (HAC) is not well suited for high-dimensional datasets.</div>
<div class="line">The computational cost (typically <span class="math notranslate nohighlight">\(O(n^2)\)</span> or worse) becomes prohibitive as the number of points grows.</div>
<div class="line">In high-dimensional spaces, the curse of dimensionality affects distance metrics ‚Äî distances between points tend to become similar, making clustering less meaningful.</div>
<div class="line">Moreover, HAC requires storing and updating a full distance matrix of size <span class="math notranslate nohighlight">\(O(n^2)\)</span>, which leads to memory inefficiencies.</div>
<div class="line">As a result, HAC is better used for small to moderately sized datasets with well-separated clusters in low-dimensional spaces.</div>
</div>
</section>
<section id="Ward-Linkage-clustering---Step-by-Step-Explanation">
<h2>Ward Linkage clustering - Step-by-Step Explanation<a class="headerlink" href="#Ward-Linkage-clustering---Step-by-Step-Explanation" title="Link to this heading">¬∂</a></h2>
<p>We will use the same 5 points with 2D coordinates.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Five 2D points</p></th>
<th class="head"><p>Coordinates</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>A</p></td>
<td><p>(1, 1)</p></td>
</tr>
<tr class="row-odd"><td><p>B</p></td>
<td><p>(2, 1)</p></td>
</tr>
<tr class="row-even"><td><p>C</p></td>
<td><p>(4, 3)</p></td>
</tr>
<tr class="row-odd"><td><p>D</p></td>
<td><p>(5, 4)</p></td>
</tr>
<tr class="row-even"><td><p>E</p></td>
<td><p>(3, 4)</p></td>
</tr>
</tbody>
</table>
<p>Step 1: Initialization Each point starts as its own cluster:</p>
<p>C1 = {A} C2 = {B} C3 = {C} C4 = {D} C5 = {E}</p>
<p><strong>Step 2: Compute the initial distances (Ward linkage).</strong></p>
<p>With Ward‚Äôs method, we look at the increase in within-cluster variance that would result from merging two clusters.</p>
<div class="line-block">
<div class="line">For singleton clusters (with one point), this step reduces to calculating squared Euclidean distances between the points,</div>
<div class="line">as it corresponds to the increase in within-cluster inertia (variance).</div>
</div>
<p>Let‚Äôs compute the squared Euclidean distances between all points:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>A</p></th>
<th class="head"><p>B</p></th>
<th class="head"><p>C</p></th>
<th class="head"><p>D</p></th>
<th class="head"><p>E</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>A</p></td>
<td><p>0</p></td>
<td><p>1.0</p></td>
<td><p>13.0</p></td>
<td><p>25.0</p></td>
<td><p>13.0</p></td>
</tr>
<tr class="row-odd"><td><p>B</p></td>
<td><p>1.0</p></td>
<td><p>0</p></td>
<td><p>8.0</p></td>
<td><p>18.0</p></td>
<td><p>10.0</p></td>
</tr>
<tr class="row-even"><td><p>C</p></td>
<td><p>13.0</p></td>
<td><p>8.0</p></td>
<td><p>0</p></td>
<td><p>2.0</p></td>
<td><p>2.0</p></td>
</tr>
<tr class="row-odd"><td><p>D</p></td>
<td><p>25.0</p></td>
<td><p>18.0</p></td>
<td><p>2.0</p></td>
<td><p>0</p></td>
<td><p>4.0</p></td>
</tr>
<tr class="row-even"><td><p>E</p></td>
<td><p>13.0</p></td>
<td><p>10.0</p></td>
<td><p>2.0</p></td>
<td><p>4.0</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line">üßÆ These are squared Euclidean distances:</div>
<div class="line">For instance, dist¬≤(A, C) = (1‚àí4)¬≤ + (1‚àí3)¬≤ = 9 + 4 = 13.</div>
</div>
<p>Step 3: First merge (min increase in variance) Smallest squared distance: C ‚Äì D = 2.0</p>
<p>‚û°Ô∏è Merge C3 + C4 ‚Üí C3‚Ä≤ = {C, D}</p>
<p>We now have:</p>
<p>C1 = {A} C2 = {B} C3‚Ä≤ = {C, D} C5 = {E} Step 4: Update distances using Ward criterion For Ward‚Äôs method, merging clusters means computing the increase in total within-cluster variance (inertia):</p>
<p>Let‚Äôs calculate the increase in variance (Œî) when merging {C, D} with other clusters.</p>
<p>We use the formula for Ward linkage:</p>
<div class="line-block">
<div class="line">Œî(Ci, Cj) = (|Ci| √ó |Cj|) / (|Ci| + |Cj|) √ó ||Œºi ‚àí Œºj||¬≤. Let‚Äôs compute the centroid Œº of {C, D}:</div>
<div class="line">Œº_CD = ((4 + 5)/2, (3 + 4)/2) = (4.5, 3.5)</div>
</div>
<p>Then compute Œî({C,D}, E):</p>
<div class="line-block">
<div class="line">Œº_E = (3, 4). ||Œº_CD ‚Äì Œº_E||¬≤ = (4.5 ‚àí 3)¬≤ + (3.5 ‚àí 4)¬≤ = 2.25 + 0.25 = 2.5</div>
<div class="line">Œî = (2√ó1)/(2+1) √ó 2.5 = 0.67 √ó 2.5 = 1.67</div>
<div class="line">Similarly, compute Œî({C,D}, B):</div>
</div>
<div class="line-block">
<div class="line">Œº_B = (2,1)</div>
<div class="line">||Œº_CD ‚Äì Œº_B||¬≤ = (2.5)¬≤ + (2.5)¬≤ = 6.25 + 6.25 = 12.5</div>
<div class="line">Œî = (2√ó1)/3 √ó 12.5 = 8.33</div>
<div class="line">And Œî({C,D}, A):</div>
</div>
<div class="line-block">
<div class="line">Œº_A = (1,1)</div>
<div class="line">||Œº_CD ‚Äì Œº_A||¬≤ = (3.5)¬≤ + (2.5)¬≤ = 12.25 + 6.25 = 18.5</div>
<div class="line">Œî = 2/3 √ó 18.5 = 12.33</div>
<div class="line">So the updated distance mtrix (increase in variance) is:</div>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>{A}</p></th>
<th class="head"><p>{B}</p></th>
<th class="head"><p>{CD}</p></th>
<th class="head"><p>{E}</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>{A}</p></td>
<td><p>0</p></td>
<td><p>1.0</p></td>
<td><p>12.33</p></td>
<td><p>13.0</p></td>
</tr>
<tr class="row-odd"><td><p>{B}</p></td>
<td><p>1.0</p></td>
<td><p>0</p></td>
<td><p>8.33</p></td>
<td><p>10.0</p></td>
</tr>
<tr class="row-even"><td><p>{CD}</p></td>
<td><p>12.33</p></td>
<td><p>8.33</p></td>
<td><p>0</p></td>
<td><p>1.67</p></td>
</tr>
<tr class="row-odd"><td><p>{E}</p></td>
<td><p>13.0</p></td>
<td><p>10.0</p></td>
<td><p>1.67</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p>Step 5: Merge {CD} and E (minimum increase = 1.67). ‚û°Ô∏è C3‚Ä≤‚Ä≤ = {C, D, E}</p>
<p>New clusters:</p>
<div class="line-block">
<div class="line">C1 = {A}</div>
<div class="line">C2 = {B}</div>
<div class="line">C3‚Ä≤‚Ä≤ = {C, D, E}</div>
</div>
<div class="line-block">
<div class="line">Step 6: Update distances again</div>
<div class="line">Compute Œî({CDE}, A):</div>
</div>
<div class="line-block">
<div class="line">Œº_CDE = ((4+5+3)/3, (3+4+4)/3) = (4, 3.67)</div>
<div class="line">Œº_A = (1,1)</div>
<div class="line">||Œº_CDE ‚Äì Œº_A||¬≤ = (3)¬≤ + (2.67)¬≤ ‚âà 9 + 7.11 = 16.11</div>
<div class="line">Œî = (3√ó1)/4 √ó 16.11 = 0.75 √ó 16.11 = 12.08</div>
<div class="line">Œî({CDE}, B):</div>
</div>
<div class="line-block">
<div class="line">Œº_B = (2,1)</div>
<div class="line">||Œº_CDE ‚Äì Œº_B||¬≤ = (2)¬≤ + (2.67)¬≤ = 4 + 7.11 = 11.11</div>
<div class="line">Œî = 0.75 √ó 11.11 = 8.33</div>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>{A}</p></th>
<th class="head"><p>{B}</p></th>
<th class="head"><p>{CDE}</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>{A}</p></td>
<td><p>0</p></td>
<td><p>1.0</p></td>
<td><p>12.08</p></td>
</tr>
<tr class="row-odd"><td><p>{B}</p></td>
<td><p>1.0</p></td>
<td><p>0</p></td>
<td><p>8.33</p></td>
</tr>
<tr class="row-even"><td><p>{CDE}</p></td>
<td><p>12.08</p></td>
<td><p>8.33</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p>‚û°Ô∏è Merge {B} and {CDE} ‚Üí C4 = {B, C, D, E}</p>
<p>Step 7: Final merge: {A} + {BCDE} Œº_BCDE = mean of points B, C, D, E = ((2+4+5+3)/4, (1+3+4+4)/4) = (3.5, 3.0) Œº_A = (1,1)</p>
<p>||Œº ‚Äì Œº||¬≤ = (2.5)¬≤ + (2.0)¬≤ = 6.25 + 4 = 10.25 Œî = (4√ó1)/5 √ó 10.25 = 0.8 √ó 10.25 = 8.2</p>
<p>üìâ Summary of Merges (Ward method)</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Step</p></th>
<th class="head"><p>Merge</p></th>
<th class="head"><p>Increase in variance (Œî)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>C + D</p></td>
<td><p>2.0</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>{CD} + E</p></td>
<td><p>1.67</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>{CDE} + B</p></td>
<td><p>8.33</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>{BCDE} + A</p></td>
<td><p>8.2</p></td>
</tr>
</tbody>
</table>
<p>Note: the final step merged at slightly lower cost than the previous due to rounding approximations.</p>
</section>
<section id="HAC-from-scratch">
<h2>HAC from scratch<a class="headerlink" href="#HAC-from-scratch" title="Link to this heading">¬∂</a></h2>
<section id="Data">
<h3>Data<a class="headerlink" href="#Data" title="Link to this heading">¬∂</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np

# Dictionnaire of the points with labels
points = {
    &quot;A&quot;: np.array([1, 1]),
    &quot;B&quot;: np.array([2, 1]),
    &quot;C&quot;: np.array([4, 3]),
    &quot;D&quot;: np.array([5, 4]),
    &quot;E&quot;: np.array([3, 4])
}
points
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;A&#39;: array([1, 1]),
 &#39;B&#39;: array([2, 1]),
 &#39;C&#39;: array([4, 3]),
 &#39;D&#39;: array([5, 4]),
 &#39;E&#39;: array([3, 4])}
</pre></div></div>
</div>
</section>
<section id="Initialization">
<h3>Initialization<a class="headerlink" href="#Initialization" title="Link to this heading">¬∂</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Initialize the clusters:
clusters = {f&quot;C{i+1}&quot;: {label} for i, label in enumerate(points.keys())}

labels = list(points.keys())
labels
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;]
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Convert to array for easier indexing
labels = list(points.keys())
X = np.array([points[label] for label in labels])
print(X)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[1 1]
 [2 1]
 [4 3]
 [5 4]
 [3 4]]
</pre></div></div>
</div>
</section>
<section id="Matrix-of-distance">
<h3>Matrix of distance<a class="headerlink" href="#Matrix-of-distance" title="Link to this heading">¬∂</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>n = len(labels)

# Initialize the matrix of distances (n x n)
dist_matrix = np.zeros((n, n))

# Calculate the euclidean distances between all the points
for i in range(n):
    for j in range(n):
        if i != j:
            pi = points[labels[i]]
            pj = points[labels[j]]
            dist = np.sqrt(np.sum((pi - pj) ** 2))
            dist_matrix[i, j] = dist


print(&quot;Distance matrix (symmetric):&quot;)
print(&quot;   &quot;, &quot;  &quot;.join(labels))
for i, row in enumerate(dist_matrix):
    print(labels[i], row)
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Distance matrix (symmetric):
    A  B  C  D  E
A [0.         1.         3.60555128 5.         3.60555128]
B [1.         0.         2.82842712 4.24264069 3.16227766]
C [3.60555128 2.82842712 0.         1.41421356 1.41421356]
D [5.         4.24264069 1.41421356 0.         2.        ]
E [3.60555128 3.16227766 1.41421356 2.         0.        ]
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from scipy.spatial.distance import euclidean

# Step 0: Initialization ‚Äî Each point is its own cluster
clusters = {i: [i] for i in range(len(X))}
history = []

def cluster_distance(c1, c2):
    # Single linkage: min distance between any point in cluster 1 and any point in cluster 2
    # we use Euclidean distance
    return min(np.sqrt(np.sum((X[i] - X[j])**2)) for i in c1 for j in c2)

step = 1
while len(clusters) &gt; 1:
    # Find the two closest clusters
    pairs = [(i, j, cluster_distance(clusters[i], clusters[j]))
             for i in clusters for j in clusters if i &lt; j]
    i_min, j_min, dist_min = min(pairs, key=lambda x: x[2])

    # Explanation step-by-step
    print(f&quot;&gt; Step {step}&quot;)
    print(f&quot;&gt; Merge clusters: {clusters[i_min]} + {clusters[j_min]} at distance {dist_min:.2f}&quot;)

    # Save history for plotting later
    history.append(dist_min)

    # Merge clusters
    new_cluster = clusters[i_min] + clusters[j_min]
    new_index = max(clusters) + 1
    clusters[new_index] = new_cluster
    del clusters[i_min]
    del clusters[j_min]

    step += 1

# === PLOT DISTANCE EVOLUTION ===

plt.figure(figsize=(6, 4))
plt.plot(range(1, len(history) + 1), history, marker=&#39;o&#39;, linestyle=&#39;-&#39;)
plt.title(&quot;Evolution of Merge Distances (Single Linkage)&quot;)
plt.xlabel(&quot;Merge Step&quot;)
plt.ylabel(&quot;Distance&quot;)
plt.grid(True)
plt.show()

# === STANDARD LINKAGE &amp; DENDROGRAM ===

Z = linkage(X, method=&#39;single&#39;, metric=&#39;euclidean&#39;)

plt.figure(figsize=(8, 4))
dendrogram(Z, labels=labels)
plt.title(&quot;Dendrogram (Single Linkage)&quot;)
plt.xlabel(&quot;Data points&quot;)
plt.ylabel(&quot;Distance&quot;)
plt.tight_layout()
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&gt; Step 1
&gt; Merge clusters: [0] + [1] at distance 1.00
&gt; Step 2
&gt; Merge clusters: [2] + [3] at distance 1.41
&gt; Step 3
&gt; Merge clusters: [4] + [2, 3] at distance 1.41
&gt; Step 4
&gt; Merge clusters: [0, 1] + [4, 2, 3] at distance 2.83
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_33_1.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_33_1.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_33_2.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_33_2.png" />
</div>
</div>
</section>
</section>
<section id="HAC-with-Python-packages">
<h2>HAC with Python packages<a class="headerlink" href="#HAC-with-Python-packages" title="Link to this heading">¬∂</a></h2>
<p>In this notebook we will build classes with spherical and equal variances.</p>
<div class="line-block">
<div class="line">Note1:</div>
<div class="line">Hierarchical Agglomerative Clustering (HAC) or simply Hierarchical Clustering in english.</div>
<div class="line">Clustering: the goal is to group data according to a coherent structure.</div>
<div class="line">Agglomerative (bottom-up) or ‚ÄúAscendante‚Äù in French because we move up the hierarchy by gradually merging clusters.</div>
<div class="line">We start from the bottom (each point alone) and move upwards (clusters get bigger and bigger).</div>
</div>
<div class="line-block">
<div class="line">Note2:</div>
<div class="line">The complexity of agglomerative hierarchical methods ranges from <span class="math notranslate nohighlight">\(O(n^2)\)</span> to <span class="math notranslate nohighlight">\(O(n^3)\)</span>,</div>
<div class="line">whereas the complexity of an exact divisive method would be <span class="math notranslate nohighlight">\(O(2^n)\)</span>. The storage complexity for the dissimilarity matrix is <span class="math notranslate nohighlight">\(O(n^2)\)</span>.</div>
<div class="line">For this reason, hierarchical clustering is generally not well suited for large-scale data.</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>##¬†HAC using Scipy
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># R code:
# cluster.colors &lt;- brewer.pal(8,&quot;Dark2&quot;)
# blobs &lt;- read.table(file=&quot;Data/blobs.txt&quot;, header=F, sep=&quot;,&quot;) ggplot(blobs, aes(x=V1, y=V2)) + geom_point()

# Install and import necessary libraries
# !pip install pandas matplotlib seaborn palettable
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from palettable.colorbrewer.qualitative import Dark2_8
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Define the cluster colors using the Dark2 palette
cluster_colors = Dark2_8.hex_colors

# Read the data from &#39;blobs.txt&#39; into a pandas DataFrame
# The file has no header and is comma-separated
path_name = &#39;/Users/davidtbo/Library/Mobile Documents/com~apple~CloudDocs/data/external&#39;
blobs = pd.read_csv(filepath_or_buffer=os.path.join(path_name,&#39;blobs.txt&#39;), header=None)

# Create a scatter plot using seaborn
# V1 (first column) is mapped to the x-axis, V2 (second column) is mapped to the y-axis
sns.scatterplot(x=blobs.iloc[:, 0], y=blobs.iloc[:, 1])

# Display the plot
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_37_0.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_37_0.png" />
</div>
</div>
</section>
<section id="HAC-using-Single-linkage-method">
<h2>HAC using Single linkage method<a class="headerlink" href="#HAC-using-Single-linkage-method" title="Link to this heading">¬∂</a></h2>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># R code:
# dend &lt;- hclust(dist(blobs[,1:2], method=&quot;euclidean&quot;), method=&quot;single&quot;)

# Minimum jump clustering
# Hierarchical clustering using the minimum jump method
# Calculates the Euclidean distance between data points (first two columns of &#39;blobs&#39;)
# Performs hierarchical clustering using the single linkage method (&#39;minimum jump&#39;)

# 1) On calcule les distances euclidienne entre les points de donn√©es (premi√®res deux colonnes de &#39;blobs&#39;)
# 2) On effectue le clustering hi√©rarchique en utilisant la m√©thode de Single linkage (minimum jump)

import scipy.cluster.hierarchy as sch
import scipy.spatial.distance as ssd

# Calculate the pairwise euclidean distance (matrix)

dist_matrix = ssd.pdist(blobs.iloc[:, 0:2], metric=&#39;euclidean&#39;)

# Perform single linkage hierarchical clustering (see the course: ML_coursDauphine_unsupervised_1_CAH_Kmeans.pdf)

# dend = sch.linkage(dist_matrix, method=&#39;single&#39;)
# dend = sch.linkage(dist_matrix, method=&#39;complete&#39;)
dend = sch.linkage(dist_matrix, method=&#39;average&#39;)

# Plot the dendrogram
plt.figure(figsize=(10, 7))
sch.dendrogram(dend)
plt.title(&#39;Dendrogram (Minimum Jump Clustering)&#39;)
plt.xlabel(&#39;Sample Index&#39;)
plt.ylabel(&#39;Distance&#39;)
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_39_0.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_39_0.png" />
</div>
</div>
<section id="Evolution-of-the-aggregation-criterion">
<h3>Evolution of the aggregation criterion<a class="headerlink" href="#Evolution-of-the-aggregation-criterion" title="Link to this heading">¬∂</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># prompt: plot(dend$height, type=&quot;b&quot;)

# Plot the evolution of the aggregation criterion
# (montre l&#39;√©volution des distances de fusion)
# The idea is to use the &quot;break&quot; (elbow, jump) in the aggregation distance curve:
# At first, the distances are small (merging of nearby clusters).
# When we start forcing the merger of very distant clusters, the distance suddenly increases.
# The goal is to cut the dendrogram just before this big jump, which gives a more &quot;natural&quot; number of clusters.
# dend[:, 2] contains the linkage distances (aggregation criterion)
plt.plot(dend[:, 2], marker=&#39;o&#39;, linestyle=&#39;-&#39;) # Use marker=&#39;o&#39; for points and linestyle=&#39;-&#39; for lines
plt.xlabel(&quot;Step&quot;)
plt.ylabel(&quot;Aggregation Criterion&quot;)
plt.title(&quot;Evolution of Aggregation Criterion&quot;)
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_41_0.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_41_0.png" />
</div>
</div>
<p>Before the ‚Äúbig‚Äù jump to 2.70, there are 3 jumps:</p>
<ol class="arabic simple">
<li><p>&lt; 1.00 small merging of points very close</p></li>
<li><p>1.00 -&gt; 1.35 first jump (merging of small clusters)</p></li>
<li><p>1.35 -&gt; 1.60 second jump (merging of clusters)</p></li>
<li><p>1.60 -&gt; 2.70 ‚Äúbig‚Äù jump (merging of groups very far from each others, the minimal euclidean distance between these far clusters s‚Äôenvole:)</p></li>
</ol>
<div class="line-block">
<div class="line">Like we do not consider the ‚Äúbig‚Äù jump, there are three clusters.</div>
<div class="line">=&gt; Choice of a partition into 3 classes</div>
</div>
</section>
<section id="Build-the-clustering">
<h3>Build the clustering<a class="headerlink" href="#Build-the-clustering" title="Link to this heading">¬∂</a></h3>
<p>Now the we know K (the number of clusters), we can use the <code class="docutils literal notranslate"><span class="pre">fcluster</span></code> function to extract the clusters from the hierarchical clustering result.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from scipy.cluster.hierarchy import fcluster

# Cut the dendrogram to get 3 clusters

clusters = fcluster(dend, 3, criterion=&#39;maxclust&#39;)

# To get a summary similar to `summary(as.factor(clusters))` in R,
# we can use pandas value_counts.
print(&quot;Cluster distribution:&quot;)
print(pd.Series(clusters).value_counts().sort_index())

# The `order_clusters_as_data = F` in R&#39;s `cutree` affects how the clusters are
# ordered in the returned vector. By default, `fcluster` returns the cluster
# assignments in the order of the original data points. This behavior is similar
# to `order_clusters_as_data = TRUE` in R.
# If you needed the clusters ordered based on the dendrogram leaves order (which is less common
# when just getting flat clusters), you would need to reorder the original data
# based on the dendrogram&#39;s leaf order before calling fcluster or reorder
# the resulting cluster array. However, for summarizing the cluster distribution,
# the order doesn&#39;t matter, and the default behavior of `fcluster` is usually what&#39;s desired
# for assigning cluster labels back to the original data points.
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Cluster distribution:
1    500
2    500
3    500
Name: count, dtype: int64
</pre></div></div>
</div>
</section>
<section id="Dendrogram-with-the-obtained-partitioning-and-clustering">
<h3>Dendrogram with the obtained partitioning and clustering<a class="headerlink" href="#Dendrogram-with-the-obtained-partitioning-and-clustering" title="Link to this heading">¬∂</a></h3>
<p>The following code performs a visual evaluation of hierarchical clustering results by displaying two plots side-by-side:</p>
<ul class="simple">
<li><p>Dendrogram: Shows the hierarchical clustering tree structure.</p></li>
<li><p>Scatter plot: Displays the original data points colored by their assigned cluster.</p></li>
</ul>
<div class="line-block">
<div class="line">Note:</div>
<div class="line">In R, the dendrogram branches are colored by cluster using color_branches, but this feature is not directly replicated in Python due to complexity.</div>
<div class="line">Instead, the Python code uses scipy to plot the dendrogram and colors the scatter plot points according to cluster assignments.</div>
<div class="line">This side-by-side visualization helps to visually assess how well the clusters correspond to the data structure.</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># R code:
# dend &lt;- color_branches(as.dendrogram(dend), clusters=clusters, col=cluster.colors[1:3])
# clusters &lt;- cutree(dend, 3, order_clusters_as_data = T)
# plot.list &lt;-list(ggplot(as.ggdend(dend)),ggplot(blobs, aes(V1,V2)) + geom_point(col=cluster.colors[clusters], size=0.2))
# ggmatrix(plot.list, nrow=1, ncol=2, showXAxisPlotLabels = F, showYAxisPlotLabels = F, xAxisLabels=c(&quot;dendrogram&quot;, &quot;scatter plot&quot;)) + theme_bw()

# !pip install plotnine mizani
# Importing necessary libraries
import matplotlib.pyplot as plt
import scipy.cluster.hierarchy as sch
import pandas as pd # Assuming blobs is a pandas DataFrame and clusters is a pandas Series or numpy array

# Assuming &#39;dend&#39; is the linkage matrix from sch.linkage
# Assuming &#39;blobs&#39; is a pandas DataFrame with the original data
# Assuming &#39;clusters&#39; is a numpy array or pandas Series containing the cluster assignments (1, 2, or 3)

# Create a figure and a set of subplots
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Plot the dendrogram in the first subplot
# We don&#39;t directly color branches based on fcluster results here as it&#39;s complex with scipy
sch.dendrogram(dend, ax=axes[0])
axes[0].set_title(&#39;Dendrogram&#39;)
axes[0].set_xlabel(&#39;Sample Index&#39;)
axes[0].set_ylabel(&#39;Distance&#39;)

# Define colors for the scatter plot based on the number of clusters
# Using a colormap and mapping cluster labels to colors
num_clusters = len(set(clusters))
cmap = plt.colormaps.get_cmap(&#39;Dark2&#39;) # Using the recommended method to get a colormap
colors = [cmap(i / (num_clusters - 1)) for i in range(num_clusters)] if num_clusters &gt; 1 else [cmap(0)]

# Plot the scatter plot with colored points based on cluster assignments in the second subplot
# Need to map cluster labels (1, 2, 3, ...) to colormap indices (0, 1, 2, ...)
# Assuming clusters are 1-indexed, subtract 1 for 0-indexed colormap access
scatter = axes[1].scatter(blobs.iloc[:, 0], blobs.iloc[:, 1], c=[colors[c-1] for c in clusters], s=5)
axes[1].set_title(&#39;Scatter Plot with Clusters&#39;)
axes[1].set_xlabel(blobs.columns[0]) # Use actual column names if available, or generic &#39;V1&#39;
axes[1].set_ylabel(blobs.columns[1]) # Use actual column names if available, or generic &#39;V2&#39;


# Adjust layout to prevent overlap
plt.tight_layout()

# Display the plots
plt.show()

# Note: The R code&#39;s `color_branches` functionality which colors the dendrogram branches
# based on the flat clustering result is not directly replicated here as it requires
# significant manipulation of the dendrogram plotting output which is beyond
# a simple conversion. The focus is on displaying the dendrogram and the
# clustered data side-by-side.
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_48_0.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_48_0.png" />
</div>
</div>
</section>
<section id="Evaluating-Cluster-Quality-Against-Ground-Truth-Labels-with-the-Confusion-Matrix">
<h3>Evaluating Cluster Quality Against Ground Truth Labels with the Confusion Matrix<a class="headerlink" href="#Evaluating-Cluster-Quality-Against-Ground-Truth-Labels-with-the-Confusion-Matrix" title="Link to this heading">¬∂</a></h3>
<div class="line-block">
<div class="line">The purpose is to check whether the clusters correspond well to the original categories,</div>
<div class="line">in order to evaluate the quality of the clustering against a ‚Äòground truth‚Äô or known label.</div>
<div class="line">This is a common analysis step after clustering to better understand the meaning of the detected groups.‚Äù</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># R code:
# table(clusters, blobs$V3)

# Assuming &#39;clusters&#39; is a numpy array or pandas Series
# Assuming &#39;blobs&#39; is a pandas DataFrame and V3 refers to the 3rd column (index 2)

# To perform the equivalent of R&#39;s `table(clusters, blobs$V3)`
# we can use pandas `crosstab` function.
# `crosstab` computes a frequency table of two (or more) factors.
# The first argument `index` corresponds to the first factor (clusters)
# The second argument `columns` corresponds to the second factor (blobs[&#39;V3&#39;])

# Ensure that the lengths of &#39;clusters&#39; and &#39;blobs&#39; are compatible.
# The number of cluster assignments should match the number of rows in blobs.
if len(clusters) == len(blobs):
    print(&quot;\nFrequency table of clusters vs. original category (blobs column V3):&quot;)
    # Use blobs.iloc[:, 2] to access the third column (V3)
    contingency_table = pd.crosstab(clusters, blobs.iloc[:, 2])
    print(contingency_table)
else:
    print(&quot;Error: The number of cluster assignments does not match the number of data points in blobs.&quot;)
<br/><br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

Frequency table of clusters vs. original category (blobs column V3):
2        0    1    2
row_0
1      500    0    0
2        0    0  500
3        0  500    0
</pre></div></div>
</div>
</section>
</section>
<section id="HAC-using-Ward's-Method">
<h2>HAC using Ward‚Äôs Method<a class="headerlink" href="#HAC-using-Ward's-Method" title="Link to this heading">¬∂</a></h2>
<p>The Ward‚Äôs method regoups two clusters that minimize the percentage of loss of variance inter-groups (Between Sum of Square) after aggregation:</p>
<p>After the aggregations of two clusters, the BSS always decrease.</p>
<ul class="simple">
<li><p>BSS general formula with the Euclidean distance is given by:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[BSS = \sum_{k=1}^K|C_k|||w_k - \bar{X}||^2\]</div>
<ul class="simple">
<li><p>BSS before regrouping A and B is given by:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[BBS_{(A,B)} = \sum_{k=1, k \neq A, B}^K |C_k|\times ||w_k - \bar{X}||^2 + C_A|\times ||w_A - \bar{X}||^2 + C_B|\times ||w_B - \bar{X}||^2\]</div>
<ul class="simple">
<li><p>BSS after regrouping A and B is given by:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[BBS_{(AB)} = \sum_{k=1, k \neq A, B}^K |C_k|\times ||w_k - \bar{X}||^2 + (C_A| + |C_B|) \times ||w_{AB} - \bar{X}||^2\]</div>
<p>At each step Ward‚Äôs method minimize: <span class="math notranslate nohighlight">\(BBS_{(A,B)} - BBS_{(AB)}\)</span></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># R code:
# dend &lt;- hclust(dist(blobs[,1:2], method=&quot;euclidean&quot;), method=&quot;ward.D2&quot;)

# Perform Ward&#39;s method hierarchical clustering
# Calculates the Euclidean distance between data points (first two columns of &#39;blobs&#39;)
# Performs hierarchical clustering using Ward&#39;s method
# `blobs.iloc[:, 0:2]` selects the first two columns of the DataFrame &#39;blobs&#39;
# `metric=&#39;euclidean&#39;` specifies the distance metric as Euclidean
# `method=&#39;ward&#39;` specifies Ward&#39;s linkage method


# Calculate the pairwise euclidean distance (matrix)

dist_matrix = ssd.pdist(blobs.iloc[:, 0:2], metric=&#39;euclidean&#39;)

dend_ward = sch.linkage(dist_matrix, method=&#39;ward&#39;)

# Plot the dendrogram for Ward&#39;s method
plt.figure(figsize=(10, 7))
sch.dendrogram(dend_ward)
plt.title(&#39;Dendrogram (Ward\&#39;s Method)&#39;)
plt.xlabel(&#39;Sample Index&#39;)
plt.ylabel(&#39;Distance&#39;)
plt.show()

# Plot the evolution of the aggregation criterion for Ward&#39;s method
plt.plot(dend_ward[:, 2], marker=&#39;o&#39;, linestyle=&#39;-&#39;)
plt.xlabel(&quot;Step&quot;)
plt.ylabel(&quot;Aggregation Criterion (Ward&#39;s)&quot;)
plt.title(&quot;Evolution of Aggregation Criterion (Ward&#39;s Method)&quot;)
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_54_0.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_54_0.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_54_1.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_54_1.png" />
</div>
</div>
<section id="Choice-of-a-partition-into-3-classes">
<h3>Choice of a partition into 3 classes<a class="headerlink" href="#Choice-of-a-partition-into-3-classes" title="Link to this heading">¬∂</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># prompt: clusters &lt;- cutree(dend, 3, order_clusters_as_data = F)
# summary(as.factor(clusters))

# Based on the dendrogram and the evolution of the aggregation criterion plot for Ward&#39;s method,
# visually identify the point where the increase in the criterion is largest.
# The comment &quot;Choix d‚Äôun d√©coupage √† 3 classes&quot; suggests that visually,
# there is a large jump in the criterion for Ward&#39;s method that would lead to choosing 3 classes.

# The equivalent R code `cutree(dend, 3, order_clusters_as_data = F)` for Ward&#39;s method
# using scipy&#39;s `fcluster` function.
# Cut the dendrogram `dend_ward` to get 3 clusters.
clusters_ward = fcluster(dend_ward, 3, criterion=&#39;maxclust&#39;)

# To get a summary similar to `summary(as.factor(clusters))` in R for Ward&#39;s method,
# use pandas value_counts.
print(&quot;\nCluster distribution (Ward&#39;s Method):&quot;)
print(pd.Series(clusters_ward).value_counts().sort_index())
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

Cluster distribution (Ward&#39;s Method):
1    500
2    500
3    500
Name: count, dtype: int64
</pre></div></div>
</div>
</section>
<section id="id1">
<h3>Dendrogram with the obtained partitioning and clustering<a class="headerlink" href="#id1" title="Link to this heading">¬∂</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># !pip install plotnine mizani
from plotnine import ggplot, aes, geom_point, theme_bw
from mizani.formatters import percent_format
from plotnine.guides import guide_legend
from plotnine.scales import scale_x_continuous, scale_y_continuous
from plotnine.labels import labs

# Import `ggmatrix` if needed, though a simple subplot approach is used below
# !pip install ggmatrix # Install if you want to use ggmatrix (less common in Python plotnine context)
# from ggmatrix import ggmatrix # Import if installed

# Import necessary libraries for plotting the dendrogram using matplotlib
import matplotlib.pyplot as plt
import scipy.cluster.hierarchy as sch
import numpy as np

# Assuming &#39;dend&#39; is the linkage matrix from sch.linkage (from the single linkage method)
# Assuming &#39;clusters&#39; is a numpy array or pandas Series containing the cluster assignments (1, 2, or 3) from single linkage
# Assuming &#39;blobs&#39; is a pandas DataFrame with the original data

# Define colors for the scatter plot based on the number of clusters
# Using the cluster_colors defined earlier (Dark2_8 palette)
# Ensure the number of colors is sufficient for the number of clusters
num_clusters = len(set(clusters))
if num_clusters &gt; len(cluster_colors):
    print(f&quot;Warning: Not enough colors defined for {num_clusters} clusters. Using repeated colors.&quot;)
    colors_for_scatter = [cluster_colors[c % len(cluster_colors)] for c in clusters]
else:
    # Map cluster labels (1, 2, 3, ...) to the defined colors (index 0, 1, 2, ...)
    colors_for_scatter = [cluster_colors[c-1] for c in clusters]


# Create a figure and a set of subplots side-by-side
fig, axes = plt.subplots(1, 2, figsize=(15, 7)) # Adjust figsize as needed

# Plot the dendrogram in the first subplot using matplotlib
# To color branches by flat clusters, this requires more advanced manipulation of the dendrogram object,
# which is not as straightforward as in R&#39;s `color_branches`. A common approach in matplotlib
# is to draw the dendrogram first and then potentially add colored lines/patches afterwards,
# or use the `color_threshold` argument if cutting by distance, but not directly by maxclust.
# For simplicity here, we plot the standard dendrogram.
sch.dendrogram(dend, ax=axes[0])
axes[0].set_title(&#39;Dendrogram&#39;)
axes[0].set_xlabel(&#39;Sample Index&#39;)
axes[0].set_ylabel(&#39;Distance&#39;)

# Plot the scatter plot with colored points based on cluster assignments in the second subplot using matplotlib
# Use the colors_for_scatter list generated based on cluster assignments
axes[1].scatter(blobs.iloc[:, 0], blobs.iloc[:, 1], c=colors_for_scatter, s=10) # Adjust size &#39;s&#39; as needed
axes[1].set_title(&#39;Scatter Plot with Clusters&#39;)
axes[1].set_xlabel(blobs.columns[0])
axes[1].set_ylabel(blobs.columns[1])

# Adjust layout to prevent overlap
plt.tight_layout()

# Display the plots
plt.show()

# Note: Directly replicating `ggmatrix` and `as.ggdend` from R in Python using plotnine
# and matplotlib is complex. The approach above uses matplotlib for both plots within
# a single figure&#39;s subplots, providing a similar side-by-side visualization.
# If you specifically need plotnine for the scatter plot, you would create a plotnine
# object for the scatter plot and display it separately or attempt to combine it
# using more advanced methods or libraries that support combining matplotlib and plotnine plots.
# The matplotlib approach for both is generally more direct when showing the dendrogram alongside.

# If you want to use plotnine for the scatter plot:
# scatter_plot = (
#     ggplot(blobs, aes(x=blobs.columns[0], y=blobs.columns[1], color=clusters.astype(str))) # Use string for discrete color
#     + geom_point(size=0.2)
#     + labs(x=&quot;V1&quot;, y=&quot;V2&quot;, color=&quot;Cluster&quot;)
#     + theme_bw()
# )

# And then display the matplotlib dendrogram and the plotnine scatter plot.
# Combining them into a single figure as `ggmatrix` does in R is not a standard
# feature of plotnine/matplotlib without significant custom code or using a
# specialized library if one exists. The matplotlib subplot approach shown above
# is the most common way to achieve side-by-side plots in Python.
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_58_0.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_58_0.png" />
</div>
</div>
</section>
<section id="Confusion-Matrix">
<h3>Confusion Matrix<a class="headerlink" href="#Confusion-Matrix" title="Link to this heading">¬∂</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># prompt: table(clusters, blobs$V3)

# The R code `table(clusters, blobs$V3)` creates a contingency table
# showing the counts of observations for each combination of `clusters`
# and the values in the third column of `blobs` (indexed as 2 in pandas).

# This was already implemented in the preceding code block.
# To reiterate the code for clarity:

# Ensure that the lengths of &#39;clusters&#39; and &#39;blobs&#39; are compatible.
if len(clusters) == len(blobs):
    print(&quot;\nFrequency table of clusters vs. original category (blobs column V3):&quot;)
    # Use blobs.iloc[:, 2] to access the third column (V3)
    contingency_table = pd.crosstab(clusters, blobs.iloc[:, 2])
    print(contingency_table)
else:
    print(&quot;Error: The number of cluster assignments does not match the number of data points in blobs.&quot;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

Frequency table of clusters vs. original category (blobs column V3):
2        0    1    2
row_0
1      500    0    0
2        0    0  500
3        0  500    0
</pre></div></div>
</div>
<p>Exercise:</p>
<div class="line-block">
<div class="line">Test the different hierarchical classification methods on the other simulated datasets (‚Äúaniso.txt‚Äù, ‚Äúaggregation.txt‚Äù, ‚Äúdifferent_density.txt‚Äù, ‚Äúnoisy_moons.txt‚Äù).</div>
<div class="line">In each case, perform clustering on the first two columns (data coordinates), and compare with the true number of classes.</div>
</div>
</section>
</section>
<section id="HAC-on-iris-data">
<h2>HAC on iris data<a class="headerlink" href="#HAC-on-iris-data" title="Link to this heading">¬∂</a></h2>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import pandas as pd
from sklearn.preprocessing import StandardScaler
import seaborn as sns
import matplotlib.pyplot as plt

# Load the IRIS dataset
# head(iris) equivalent in pandas
iris = sns.load_dataset(&#39;iris&#39;)
print(iris.head())

# Normalization (zero mean, unit variance)
# Equivalent to iris.norm &lt;- data.frame(sapply(iris[,1:4], scale)) in R
scaler = StandardScaler()
iris_norm_data = scaler.fit_transform(iris.iloc[:, 0:4])
iris_norm = pd.DataFrame(iris_norm_data, columns=iris.columns[0:4])

# Add the &#39;Species&#39; column back to the normalized dataframe
# Equivalent to iris.norm$Species &lt;- iris$Species
iris_norm[&#39;species&#39;] = iris[&#39;species&#39;]

# Scatter plot matrices and distribution of classes by variable
# Equivalent to ggpairs(iris, columns=1:4, aes(color=Species)) using seaborn&#39;s pairplot
# Note: ggpairs from R&#39;s GGally is more comprehensive, pairplot is a common alternative in Python
sns.pairplot(iris, hue=&quot;species&quot;)
plt.suptitle(&quot;Scatter plot matrix and distribution of classes by variable for IRIS dataset&quot;, y=1.02) # Add a title
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
   sepal_length  sepal_width  petal_length  petal_width species
0           5.1          3.5           1.4          0.2  setosa
1           4.9          3.0           1.4          0.2  setosa
2           4.7          3.2           1.3          0.2  setosa
3           4.6          3.1           1.5          0.2  setosa
4           5.0          3.6           1.4          0.2  setosa
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_63_1.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_63_1.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from scipy.cluster.hierarchy import dendrogram, linkage, cut_tree
import numpy as np

# Hierarchical Classification with Ward&#39;s method
# Equivalent to dend &lt;- hclust(dist(iris.norm[,1:4], method=&quot;euclidean&quot;), method=&quot;ward.D2&quot;) in R
linked = linkage(iris_norm.iloc[:, 0:4], method=&#39;ward&#39;)

# Dendrogram
# Equivalent to plot(dend) in R
plt.figure(figsize=(10, 7))
dendrogram(linked)
plt.title(&quot;Dendrogram of IRIS dataset&quot;)
plt.xlabel(&quot;Data points&quot;)
plt.ylabel(&quot;Distance&quot;)
plt.show()

# Evolution of the aggregation criterion
# Equivalent to plot(dend$height, type=&quot;b&quot;) in R
plt.figure(figsize=(10, 7))
plt.plot(linked[:, 2], marker=&#39;o&#39;)
plt.title(&quot;Evolution of the aggregation criterion&quot;)
plt.xlabel(&quot;Number of merges&quot;)
plt.ylabel(&quot;Distance&quot;)
plt.show()


# Choosing a cut at 5 classes
# Equivalent to clusters &lt;- cutree(dend, 5, order_clusters_as_data = F) in R
# Note: cutree in scipy does not have an equivalent to order_clusters_as_data = F,
# the clusters are assigned in the order of the original data.
clusters = cut_tree(linked, n_clusters=5).flatten()

# Summary of the number of elements in each cluster
# Equivalent to summary(as.factor(clusters)) in R
unique_clusters, counts = np.unique(clusters, return_counts=True)
print(&quot;Cluster distribution:&quot;)
for cluster_id, count in zip(unique_clusters, counts):
    print(f&quot;Cluster {cluster_id}: {count}&quot;)

# Dendrogram with the partitioning and obtained clustering
# Equivalent to dend &lt;- color_branches(as.dendrogram(dend), clusters=clusters, col=cluster.colors[1:5]) in R
# and the subsequent plotting code.
# Coloring branches in matplotlib dendrogram is more involved than in R&#39;s dendextend.
# We&#39;ll replot the dendrogram and potentially add labels/colors manually if needed.
plt.figure(figsize=(10, 7))
dendrogram(
    linked,
    leaf_rotation=90.,
    leaf_font_size=8.,
    labels=clusters # Using cluster labels as leaf labels for visualization
)
plt.title(&quot;Dendrogram of IRIS dataset with 5 Clusters&quot;)
plt.xlabel(&quot;Data points (colored by cluster)&quot;)
plt.ylabel(&quot;Distance&quot;)
plt.show()

# Scatter plot with clustering results
# Equivalent to ggplot(iris, aes(Petal.Length, Petal.Width)) + geom_point(col=cluster.colors[clusters], size=1))
plt.figure(figsize=(8, 6))
scatter = plt.scatter(iris[&#39;petal_length&#39;], iris[&#39;petal_width&#39;], c=clusters, cmap=&#39;viridis&#39;, s=10)
plt.title(&quot;Petal Length vs Petal Width colored by Cluster&quot;)
plt.xlabel(&quot;Petal Length&quot;)
plt.ylabel(&quot;Petal Width&quot;)
plt.colorbar(scatter, label=&#39;Cluster ID&#39;)
plt.show()

# Creating a combined plot (Equivalent to ggmatrix part)
# This requires more custom plotting. We&#39;ll show the two plots separately as direct equivalent of ggmatrix isn&#39;t straightforward in basic matplotlib/seaborn.

# Confusion matrix
# Equivalent to table(clusters, iris$Species) in R
from sklearn.metrics import confusion_matrix
# Need to map the species names to numerical labels to compare with cluster IDs
species_map = {species: i for i, species in enumerate(iris[&#39;species&#39;].unique())}
true_labels = iris[&#39;species&#39;].map(species_map)

conf_matrix = confusion_matrix(true_labels, clusters)
print(&quot;\nConfusion Matrix (True Species vs Clusters):&quot;)
conf_matrix

# For a more detailed comparison, you might want to see how each cluster relates to the original species.
# Note that cluster IDs from cut_tree are arbitrary and don&#39;t necessarily correspond to the original species labels.
# You would typically assign cluster labels to the majority species in each cluster for interpretation.
<br/></pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_64_0.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_64_0.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_64_1.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_64_1.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Cluster distribution:
Cluster 0: 29
Cluster 1: 20
Cluster 2: 30
Cluster 3: 45
Cluster 4: 26
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_64_3.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_64_3.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_64_4.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_1_CAH_64_4.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

Confusion Matrix (True Species vs Clusters):
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([[29, 20,  1,  0,  0],
       [ 0,  0, 27, 23,  0],
       [ 0,  0,  2, 22, 26],
       [ 0,  0,  0,  0,  0],
       [ 0,  0,  0,  0,  0]])
</pre></div></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../index.html" class="btn btn-neutral float-left" title="Unsupervised learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ML_labDauphine_unsupervised_2_Kmeans.html" class="btn btn-neutral float-right" title="Clustering: KMeans" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, David TBO.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>