

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Clustering: Gaussian Mixed Mixture (GMM) &mdash; Machine Learning and Deep Learning 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/nbsphinx-code-cells.css?v=2aa19091" />

  
      <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js?v=e031e9a9"></script>
      <script src="../../../../_static/doctools.js?v=888ff710"></script>
      <script src="../../../../_static/sphinx_highlight.js?v=4825356b"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Clustering: ISOMAP" href="ML_labDauphine_unsupervised_5_ISOMAP.html" />
    <link rel="prev" title="Clustering: DBSCAN" href="ML_labDauphine_unsupervised_3_DBSCAN.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            Machine Learning and Deep Learning
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../index.html">Machine Learning</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../index.html#notebooks">Notebooks</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../index.html">Unsupervised learning</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="../index.html#notebooks">Notebooks</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">Machine Learning and Deep Learning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Machine Learning</a></li>
          <li class="breadcrumb-item"><a href="../index.html">Unsupervised learning</a></li>
      <li class="breadcrumb-item active">Clustering: Gaussian Mixed Mixture (GMM)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../_sources/AI/machine_learning/unsupervised_learning/notebooks/ML_labDauphine_unsupervised_4_GMM.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Clustering:-Gaussian-Mixed-Mixture-(GMM)">
<h1>Clustering: Gaussian Mixed Mixture (GMM)<a class="headerlink" href="#Clustering:-Gaussian-Mixed-Mixture-(GMM)" title="Permalink to this heading">¬∂</a></h1>
<p>üéØ <strong>Goal</strong></p>
<div class="line-block">
<div class="line">We want to <strong>approximate the distribution of the data as a mixture of two Gaussians (K = 2).</strong></div>
<div class="line">Instead of saying ‚Äúthis point is in cluster 1‚Äù, we say this point <strong>probably</strong> came from <em>Gaussian1</em> with probability <span class="math notranslate nohighlight">\(\gamma\)</span>‚Ä¶</div>
</div>
<p>We‚Äôll use the <strong>Expectation-Maximization (EM)</strong> algorithm to learn the model.</p>
<p>The dataset:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Point</p></th>
<th class="head"><p>Coordinates</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>A</p></td>
<td><p>(1, 1)</p></td>
</tr>
<tr class="row-odd"><td><p>B</p></td>
<td><p>(2, 1)</p></td>
</tr>
<tr class="row-even"><td><p>C</p></td>
<td><p>(4, 3)</p></td>
</tr>
<tr class="row-odd"><td><p>D</p></td>
<td><p>(5, 4)</p></td>
</tr>
<tr class="row-even"><td><p>E</p></td>
<td><p>(3, 4)</p></td>
</tr>
</tbody>
</table>
<p>üìå <strong>Step 1: Initialization of parameters</strong></p>
<p><strong>We assume the data comes from a mixture of 2 Gaussian components (K = 2)</strong>. We initialize:</p>
<ul class="simple">
<li><p>Means <span class="math notranslate nohighlight">\((\mu)\)</span></p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\mu_1 = (1, 1), \mu_2 = (5, 4) \leftarrow\)</span> based on points A and D.</p>
<ul class="simple">
<li><p>Covariance <span class="math notranslate nohighlight">\((\Sigma)\)</span></p></li>
</ul>
<p>$:nbsphinx-math:<cite>Sigma</cite>_1 = <span class="math">\Sigma</span>_2 = $ identity matrix (simplified asumption)</p>
<ul class="simple">
<li><p>Mixing weight <span class="math notranslate nohighlight">\((\pi)\)</span> equal priors:</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\pi_1 = 0.5, \pi_2 = 0.5\)</span>.</p>
<p>üîÑ <strong>Step 2: E-step (Expectation)</strong></p>
<p>For each point <span class="math notranslate nohighlight">\(x_i\)</span>, compute the <strong>responsibility</strong> <span class="math notranslate nohighlight">\(\gamma_{i_k}\)</span>, which is the <strong>posterior probability</strong> that the point <span class="math notranslate nohighlight">\(i\)</span> belongs to cluster <span class="math notranslate nohighlight">\(k\)</span> given the current parameters <span class="math notranslate nohighlight">\(\pi_k, \mu_k, \Sigma_k\)</span>:</p>
<div class="math notranslate nohighlight">
\[\gamma_{ik} = P(z_i=k | x_i)\]</div>
<div class="math notranslate nohighlight">
\[\gamma_{ik} = \dfrac{\pi_k.N(x_i | \mu_k, \Sigma_k)}{\Sigma_{j=1}^K\pi_i.N(x_i | \mu_j, \Sigma_j)}\]</div>
<p>Where <span class="math notranslate nohighlight">\(N(x_i | \mu_k, \Sigma_k)\)</span> is the multivariate Gaussian likelihood of <span class="math notranslate nohighlight">\(x_i\)</span> under cluster <span class="math notranslate nohighlight">\(k\)</span> and given by <span class="math notranslate nohighlight">\(\dfrac{1}{(2\pi)^{d/2}|\Sigma_k|^{1/2}}.exp(-\dfrac{1}{2} (x_i - \mu_j)^T\Sigma_k^{-1}(x_i - \mu_j))\)</span> with <span class="math notranslate nohighlight">\(d\)</span> the dimensionality of <span class="math notranslate nohighlight">\(x_i\)</span>.</p>
<p>Example: compute for the Point <span class="math notranslate nohighlight">\(B:(2,1)\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\|\ B - \mu_1 \|\ ^2 = (2-1)^2 + (1-1)^2 = 1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\|\ B - \mu_2 \|\ ^2 = (2-5)^2 + (1-4)^2 = 9 + 9 = 18\)</span>
Then:
<span class="math notranslate nohighlight">\(\gamma_{B,1} = \dfrac{exp(-\dfrac{1}{2} \times 1)}{exp(-\dfrac{1}{2} \times 1) + exp(-\dfrac{1}{2} \times 18)} \approx \dfrac{0.607}{0.607 + 0.0001} \approx 0.607 &gt; 0.5\)</span></p></li>
</ul>
<p>So in the table: <span class="math notranslate nohighlight">\(B \rightarrow (\gamma_1 \approx 0.61, \gamma_2 \approx 0.39)\)</span></p>
<p>After computing responsibilities (approximate values):</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Point</p></th>
<th class="head"><p>Œ≥ (Cluster 1)</p></th>
<th class="head"><p>Œ≥ (Cluster 2)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>A</p></td>
<td><p>~1.00</p></td>
<td><p>~0.00</p></td>
</tr>
<tr class="row-odd"><td><p>B</p></td>
<td><p>~0.67</p></td>
<td><p>~0.39</p></td>
</tr>
<tr class="row-even"><td><p>C</p></td>
<td><p>~0.30</p></td>
<td><p>~0.70</p></td>
</tr>
<tr class="row-odd"><td><p>D</p></td>
<td><p>~0.00</p></td>
<td><p>~1.00</p></td>
</tr>
<tr class="row-even"><td><p>E</p></td>
<td><p>~0.10</p></td>
<td><p>~0.90</p></td>
</tr>
</tbody>
</table>
<p>(Simplified values based on proximity to cluster means).</p>
<div class="line-block">
<div class="line">This E-step gives <strong>soft assignment</strong> (probabilities).</div>
<div class="line">E-step uses the current parameters - it doesn‚Äôt improve them. It just tells you, given the current model, what the likely cluster memberships are. That‚Äôs not enough. Your goal is to fit the best model to the data - and that means improving the parameters (M-step).</div>
</div>
<p>üìà <strong>Step 3: M-step (Maximization)</strong>.</p>
<p>Now update the parameters based on the responsibilities (probabilities) of the E-step:</p>
<div class="line-block">
<div class="line"><strong>Context:</strong></div>
<div class="line">In soft clustering (as in GMM), each data point belongs partially to all clusters, with a degree of membership given by the responsibility <span class="math notranslate nohighlight">\(\gamma_{ik}\)</span>. So, instead of counting how many points are assigned to a cluster (as in K-means), we sum the partial responsibilities of each point for that cluster (<span class="math notranslate nohighlight">\(N_k = \sum_{i=1}^N\gamma_{ik}\)</span>).</div>
</div>
<ul class="simple">
<li><p>Updated means:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mu_k = \dfrac{\sum_{i=1}^N\gamma_{ik}.x_i}{\sum_{i=1}^N\gamma_{ik}} = \dfrac{\sum_{i=1}^N\gamma_{ik}.x_i}{N_k}\]</div>
<ul class="simple">
<li><p>Updated covariances:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\Sigma_k = \dfrac{\sum_{i=1}^N\gamma_{ik}.(x_i - \mu_k)(x_i - \mu_k)^T}{\sum_{i=1}^N\gamma_{ik}} = \dfrac{\sum_{i=1}^N\gamma_{ik}.(x_i - \mu_k)(x_i - \mu_k)^T}{N_k}\]</div>
<ul class="simple">
<li><p>Updated mixing weights (priors):</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\pi_k = \dfrac{1}{N}\sum_{i=1}^N\gamma_{ik} = \dfrac{N_k}{N}\]</div>
<p>Example: updating <span class="math notranslate nohighlight">\(\mu_1\)</span></p>
<p><span class="math notranslate nohighlight">\(\mu_1 = \dfrac{1 \times A + 0.67 \times B + 0.3 \times C + 0 \times D + 0.1 \times E}{1 + 0.67 + 0.3 + 0 + 0.1}\)</span></p>
<p>This new mean will shift slightly toward <span class="math notranslate nohighlight">\(B\)</span> and <span class="math notranslate nohighlight">\(C\)</span>.</p>
<p>üîÅ <strong>Step 4: Iterate E-step and M-step</strong></p>
<p>Repeat steps 2 and 3 until convergence (i.e., the parameters change become very little between iterations).</p>
<p>With each iteration:</p>
<ul class="simple">
<li><p>The responsibilities become sharper (closer to 0 or 1).</p></li>
<li><p>The means settle into two natural centers.</p></li>
<li><p>The weights <span class="math notranslate nohighlight">\(\pi_1\)</span> and <span class="math notranslate nohighlight">\(\pi_2\)</span> reflect the size of each cluster.</p></li>
</ul>
<p>‚úÖ <strong>Step 5: Final Cluster Assignment</strong></p>
<p>After convergence, we can:</p>
<ul class="simple">
<li><p>Keep the responsibilities for soft clustering, or</p></li>
<li><p>Assign each point to the cluster with the highest Œ≥ (hard clustering)</p></li>
</ul>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Point</p></th>
<th class="head"><p>Assigned Cluster (argmax Œ≥)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>A</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>B</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>C</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-odd"><td><p>D</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-even"><td><p>E</p></td>
<td><p>2</p></td>
</tr>
</tbody>
</table>
<p><span class="math notranslate nohighlight">\(\rightarrow\)</span> This matches the intuitive groupings: (A, B) vs. (C, D, E).</p>
<p>üìå <strong>Summary of GMM (EM Algorithm)</strong></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Step</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Init</p></td>
<td><p>Choose initial values for <span class="math notranslate nohighlight">\(\mu_k\)</span>, <span class="math notranslate nohighlight">\(\Sigma_k\)</span>, and <span class="math notranslate nohighlight">\(\pi_k\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>E-step</p></td>
<td><p>Compute responsibilities <span class="math notranslate nohighlight">\(\gamma_{ik}\)</span> for all data points</p></td>
</tr>
<tr class="row-even"><td><p>M-step</p></td>
<td><p>Update the parameters using the responsibilities</p></td>
</tr>
<tr class="row-odd"><td><p>Repeat</p></td>
<td><p>Until convergence of parameters</p></td>
</tr>
<tr class="row-even"><td><p>Output</p></td>
<td><p>Use soft or hard assignments to define clusters</p></td>
</tr>
</tbody>
</table>
<section id="Example-with-basic-Python-codes">
<h2>Example with basic Python codes<a class="headerlink" href="#Example-with-basic-Python-codes" title="Permalink to this heading">¬∂</a></h2>
<p>üìå <strong>Step 1: Initialization of parameters</strong></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[89]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Load the dataset</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># number of clusters</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1">#¬†randomly initialize the means by selecting K random points from the dataset</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="n">K</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>  <span class="c1"># shape (2, 4)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initial means:&quot;</span><span class="p">)</span>
<span class="n">mu1</span> <span class="o">=</span> <span class="n">mu</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;mu1 : </span><span class="si">{</span><span class="n">mu1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">mu2</span> <span class="o">=</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;mu2 : </span><span class="si">{</span><span class="n">mu2</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># covariance matrices (identity matrices for simplicity)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initial covariance matrix:&quot;</span><span class="p">)</span>
<span class="n">cov1</span> <span class="o">=</span> <span class="n">cov2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cov1</span><span class="p">)</span>

<span class="n">pi1</span> <span class="o">=</span> <span class="n">pi2</span> <span class="o">=</span> <span class="mf">.5</span>  <span class="c1"># equal priors for both clusters</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initial weights (priors):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;pi1 : </span><span class="si">{</span><span class="n">pi1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;pi2 : </span><span class="si">{</span><span class="n">pi2</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[1 1]
 [2 1]
 [4 3]
 [5 4]
 [3 4]]
Initial means:
mu1 : [[1 1]]
mu2 : [[5 4]]
Initial covariance matrix:
[[1. 0.]
 [0. 1.]]
Initial weights (priors):
pi1 : 0.5
pi2 : 0.5
</pre></div></div>
</div>
<p>üîÑ <strong>Step 2: E-step (Expectation)</strong></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[90]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#¬†E-step: calculate responsibilities</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span>

<span class="c1"># Define the Gaussians</span>
<span class="n">rv1</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu1</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov1</span><span class="p">)</span>
<span class="n">rv2</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu2</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov2</span><span class="p">)</span>

<span class="n">gammas</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># list to store responsibilities</span>

<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">:</span>
    <span class="c1"># Compute likelihood under each Gaussian</span>
    <span class="n">p1</span> <span class="o">=</span> <span class="n">pi1</span> <span class="o">*</span> <span class="n">rv1</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># = joint proba to observe x and x is in the cluster 1 = prior √ó likelihood = responsibility numerator for cluster 1</span>
    <span class="n">p2</span> <span class="o">=</span> <span class="n">pi2</span> <span class="o">*</span> <span class="n">rv2</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># = joint proba to observe x and x is in the cluster 2 = prior √ó likelihood = responsibility numerator for cluster 2</span>

    <span class="c1"># Normalize to get gamma (posterior probability for cluster 1)</span>
    <span class="n">gamma1</span> <span class="o">=</span> <span class="n">p1</span> <span class="o">/</span> <span class="p">(</span><span class="n">p1</span> <span class="o">+</span> <span class="n">p2</span><span class="p">)</span> <span class="c1"># if gamma1 is above 0.5, then x is more likely to belong to cluster 1</span>
    <span class="n">gamma2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">gamma1</span>
    <span class="n">gammas</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">gamma1</span><span class="p">,</span> <span class="n">gamma2</span><span class="p">])</span>

<span class="c1"># Convert to array for easier inspection</span>
<span class="n">gammas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">gammas</span><span class="p">)</span>

<span class="c1"># Show results</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">gammas</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Gamma (Cluster 1)&quot;</span><span class="p">,</span> <span class="s2">&quot;Gamma (Cluster 2)&quot;</span><span class="p">])</span>
<span class="n">df</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Point&quot;</span><span class="p">,</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;X</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="c1">#¬†Extra: two ways to calculate the probability of a point belonging to cluster 1 P(x_i | C_1))</span>
<span class="c1"># 1. Using the formula for multivariate Gaussian distribution</span>
<span class="n">prob1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">**</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">cov1</span><span class="p">)))</span> <span class="o">*</span> \
    <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">X</span> <span class="o">-</span> <span class="n">mu1</span><span class="p">)</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">cov1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">mu1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Probability for cluster 1: </span><span class="si">{</span><span class="n">prob1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 2. Using scipy&#39;s multivariate_normal for the same calculation</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span>
<span class="n">rv1</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu1</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov1</span><span class="p">)</span>
<span class="n">prob1</span> <span class="o">=</span> <span class="n">rv1</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Probability for cluster 1: </span><span class="si">{</span><span class="n">prob1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
  Point  Gamma (Cluster 1)  Gamma (Cluster 2)
0    X1           0.999996           0.000004
1    X2           0.999797           0.000203
2    X3           0.004070           0.995930
3    X4           0.000004           0.999996
4    X5           0.010987           0.989013
Probability for cluster 1: [1.59154943e-01 9.65323526e-02 2.39279779e-04 5.93115274e-07
 2.39279779e-04]
Probability for cluster 1: [1.59154943e-01 9.65323526e-02 2.39279779e-04 5.93115274e-07
 2.39279779e-04]
</pre></div></div>
</div>
<p>üîÅ <strong>Step 3: M-step (Maximization)</strong></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[91]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- M-step ---&quot;</span><span class="p">)</span>
<span class="n">gamma1</span> <span class="o">=</span> <span class="n">gammas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Reshape for matrix operations</span>
<span class="n">gamma2</span> <span class="o">=</span> <span class="n">gammas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">N1</span> <span class="o">=</span> <span class="n">gamma1</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">N2</span> <span class="o">=</span> <span class="n">gamma2</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="c1"># Updated means</span>
<span class="n">mu1_new</span> <span class="o">=</span> <span class="p">(</span><span class="n">gamma1</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">N1</span>
<span class="n">mu2_new</span> <span class="o">=</span> <span class="p">(</span><span class="n">gamma2</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">N2</span>

<span class="c1"># Updated covariances</span>
<span class="n">cov1_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">cov2_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>
    <span class="n">diff1</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">mu1_new</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">diff2</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">mu2_new</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">cov1_new</span> <span class="o">+=</span> <span class="n">gamma1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">diff1</span> <span class="o">@</span> <span class="n">diff1</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">cov2_new</span> <span class="o">+=</span> <span class="n">gamma2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">diff2</span> <span class="o">@</span> <span class="n">diff2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

<span class="n">cov1_new</span> <span class="o">/=</span> <span class="n">N1</span>
<span class="n">cov2_new</span> <span class="o">/=</span> <span class="n">N2</span>

<span class="c1"># Updated weights. It&#39;s the MLE estimate for the priors.</span>
<span class="n">pi1_new</span> <span class="o">=</span> <span class="n">N1</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">pi2_new</span> <span class="o">=</span> <span class="n">N2</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Show updated parameters</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- M-step ---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Updated means:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;mu1_new: </span><span class="si">{</span><span class="n">mu1_new</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;mu2_new: </span><span class="si">{</span><span class="n">mu2_new</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Updated covariances:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;cov1_new:</span><span class="se">\n</span><span class="si">{</span><span class="n">cov1_new</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;cov2_new:</span><span class="se">\n</span><span class="si">{</span><span class="n">cov2_new</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Updated priors:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;pi1_new: </span><span class="si">{</span><span class="n">pi1_new</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;pi2_new: </span><span class="si">{</span><span class="n">pi2_new</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

--- M-step ---

--- M-step ---
Updated means:
mu1_new: [1.51318654 1.0204046 ]
mu2_new: [4.00353925 3.66616333]

Updated covariances:
cov1_new:
[[0.27287465 0.03438906]
 [0.03438906 0.05675732]]
cov2_new:
[[0.66657341 0.00160165]
 [0.00160165 0.22280612]]

Updated priors:
pi1_new: 0.4029707306736868
pi2_new: 0.5970292693263132
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[95]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span>

<span class="c1"># Dataset</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>

<span class="c1"># Initialization</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="n">K</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">mu1</span> <span class="o">=</span> <span class="n">mu</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span>
<span class="n">mu2</span> <span class="o">=</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
<span class="n">cov1</span> <span class="o">=</span> <span class="n">cov2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">pi1</span> <span class="o">=</span> <span class="n">pi2</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="c1"># EM loop</span>
<span class="n">max_iter</span> <span class="o">=</span> <span class="mi">10</span>
<span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
    <span class="c1"># E-step</span>
    <span class="n">rv1</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu1</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov1</span><span class="p">,</span> <span class="n">allow_singular</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">rv2</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu2</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov2</span><span class="p">,</span> <span class="n">allow_singular</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">gammas</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">:</span>
        <span class="n">p1</span> <span class="o">=</span> <span class="n">pi1</span> <span class="o">*</span> <span class="n">rv1</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">p2</span> <span class="o">=</span> <span class="n">pi2</span> <span class="o">*</span> <span class="n">rv2</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">gamma1</span> <span class="o">=</span> <span class="n">p1</span> <span class="o">/</span> <span class="p">(</span><span class="n">p1</span> <span class="o">+</span> <span class="n">p2</span><span class="p">)</span>
        <span class="n">gamma2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">gamma1</span>
        <span class="n">gammas</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">gamma1</span><span class="p">,</span> <span class="n">gamma2</span><span class="p">])</span>
    <span class="n">gammas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">gammas</span><span class="p">)</span>

    <span class="c1"># M-step</span>
    <span class="n">gamma1</span> <span class="o">=</span> <span class="n">gammas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">gamma2</span> <span class="o">=</span> <span class="n">gammas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">N1</span> <span class="o">=</span> <span class="n">gamma1</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">N2</span> <span class="o">=</span> <span class="n">gamma2</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="n">mu1_new</span> <span class="o">=</span> <span class="p">(</span><span class="n">gamma1</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">N1</span>
    <span class="n">mu2_new</span> <span class="o">=</span> <span class="p">(</span><span class="n">gamma2</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">N2</span>

    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-6</span>
    <span class="n">cov1_new</span> <span class="o">+=</span> <span class="n">epsilon</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">cov2_new</span> <span class="o">+=</span> <span class="n">epsilon</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>
        <span class="n">diff1</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">mu1_new</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">diff2</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">mu2_new</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">cov1_new</span> <span class="o">+=</span> <span class="n">gamma1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">diff1</span> <span class="o">@</span> <span class="n">diff1</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">cov2_new</span> <span class="o">+=</span> <span class="n">gamma2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">diff2</span> <span class="o">@</span> <span class="n">diff2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="n">cov1_new</span> <span class="o">/=</span> <span class="n">N1</span>
    <span class="n">cov2_new</span> <span class="o">/=</span> <span class="n">N2</span>

    <span class="n">pi1_new</span> <span class="o">=</span> <span class="n">N1</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">pi2_new</span> <span class="o">=</span> <span class="n">N2</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1"># Update parameters for next iteration</span>
    <span class="n">mu1</span><span class="p">,</span> <span class="n">mu2</span> <span class="o">=</span> <span class="n">mu1_new</span><span class="p">,</span> <span class="n">mu2_new</span>
    <span class="n">cov1</span><span class="p">,</span> <span class="n">cov2</span> <span class="o">=</span> <span class="n">cov1_new</span><span class="p">,</span> <span class="n">cov2_new</span>
    <span class="n">pi1</span><span class="p">,</span> <span class="n">pi2</span> <span class="o">=</span> <span class="n">pi1_new</span><span class="p">,</span> <span class="n">pi2_new</span>

<span class="c1"># -------------------------------</span>
<span class="c1"># Final results display</span>
<span class="c1"># -------------------------------</span>

<span class="c1"># Table of responsibilities</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">gammas</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Gamma (Cluster 1)&quot;</span><span class="p">,</span> <span class="s2">&quot;Gamma (Cluster 2)&quot;</span><span class="p">])</span>
<span class="n">df</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Point&quot;</span><span class="p">,</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;X</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Final responsibilities (posterior probabilities):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="c1"># Probability of a point belonging to cluster 1 ‚Äî manual method</span>
<span class="n">prob1_manual</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">**</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">cov1</span><span class="p">)))</span> <span class="o">*</span> \
    <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">X</span> <span class="o">-</span> <span class="n">mu1</span><span class="p">)</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">cov1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">mu1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Probability of belonging to cluster 1 (manual formula):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">prob1_manual</span><span class="p">)</span>

<span class="c1"># Probability of a point belonging to cluster 1 ‚Äî scipy method</span>
<span class="n">rv1</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu1</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov1</span><span class="p">)</span>
<span class="n">prob1_scipy</span> <span class="o">=</span> <span class="n">rv1</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Probability of belonging to cluster 1 (scipy):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">prob1_scipy</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

Final responsibilities (posterior probabilities):
  Point  Gamma (Cluster 1)  Gamma (Cluster 2)
0    X1                1.0       9.111665e-09
1    X2                1.0       1.110065e-07
2    X3                0.0       1.000000e+00
3    X4                0.0       1.000000e+00
4    X5                0.0       1.000000e+00

Probability of belonging to cluster 1 (manual formula):
[16.57636077 16.57635993  0.          0.          0.        ]

Probability of belonging to cluster 1 (scipy):
[16.57636077 16.57635993  0.          0.          0.        ]
</pre></div></div>
</div>
</section>
<section id="Example-with-sklearn-on-the-Iris-dataset">
<h2>Example with sklearn on the Iris dataset<a class="headerlink" href="#Example-with-sklearn-on-the-Iris-dataset" title="Permalink to this heading">¬∂</a></h2>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[92]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Load the iris dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
<span class="n">species</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target_names</span>

<span class="c1"># Fit Gaussian Mixture Models with 1 to 10 components and compute BIC</span>
<span class="n">bics</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1">#¬†Bayesian Information Criterion, BIC = -2log(L) + k log(n), where k is the number of parameters, n is the number of data points and L is the likelihood of the model given the data.</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">):</span>
    <span class="n">gmm</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">gmm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">bics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gmm</span><span class="o">.</span><span class="n">bic</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
    <span class="n">models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gmm</span><span class="p">)</span>

<span class="c1"># Plot BIC values</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">),</span> <span class="n">bics</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of components&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;BIC&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;BIC for GMM on Iris dataset&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># According to BIC, the best model has 2 components</span>
<span class="n">best_n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">bics</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best number of components according to BIC: </span><span class="si">{</span><span class="n">best_n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Fit GMM with 2 components</span>
<span class="n">gmm_best</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">best_n</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">gmm_best</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">gmm_best</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Contingency table between predicted classes and true species</span>
<span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rownames</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Predicted&#39;</span><span class="p">],</span> <span class="n">colnames</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;True Species&#39;</span><span class="p">])</span>

<span class="c1"># If we force a model with 3 components (as in the R code)</span>
<span class="n">gmm_3</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">gmm_3</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">labels_3</span> <span class="o">=</span> <span class="n">gmm_3</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Contingency table for 3 components</span>
<span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">labels_3</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rownames</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Predicted (3 classes)&#39;</span><span class="p">],</span> <span class="n">colnames</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;True Species&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_4_GMM_15_0.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_4_GMM_15_0.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Best number of components according to BIC: 2
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[92]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>True Species</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
    </tr>
    <tr>
      <th>Predicted (3 classes)</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>45</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>50</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>5</td>
      <td>50</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p><strong>END</strong></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="ML_labDauphine_unsupervised_3_DBSCAN.html" class="btn btn-neutral float-left" title="Clustering: DBSCAN" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ML_labDauphine_unsupervised_5_ISOMAP.html" class="btn btn-neutral float-right" title="Clustering: ISOMAP" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, David TBO.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>