

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Clustering: Gaussian Mixed Mixture (GMM) &mdash; Machine Learning and Deep Learning 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/nbsphinx-code-cells.css?v=2aa19091" />

  
      <script src="../../../../_static/documentation_options.js?v=2709fde1"></script>
      <script src="../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Clustering: ISOMAP" href="ML_labDauphine_unsupervised_5_ISOMAP.html" />
    <link rel="prev" title="Clustering: DBSCAN" href="ML_labDauphine_unsupervised_3_DBSCAN.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            Machine Learning and Deep Learning
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../index.html">Machine Learning</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../index.html#notebooks">Notebooks</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../index.html">Unsupervised learning</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="../index.html#notebooks">Notebooks</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">Machine Learning and Deep Learning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Machine Learning</a></li>
          <li class="breadcrumb-item"><a href="../index.html">Unsupervised learning</a></li>
      <li class="breadcrumb-item active">Clustering: Gaussian Mixed Mixture (GMM)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../_sources/AI/machine_learning/unsupervised_learning/notebooks/ML_labDauphine_unsupervised_4_GMM.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Clustering:-Gaussian-Mixed-Mixture-(GMM)">
<h1>Clustering: Gaussian Mixed Mixture (GMM)<a class="headerlink" href="#Clustering:-Gaussian-Mixed-Mixture-(GMM)" title="Link to this heading">¬∂</a></h1>
<p>üéØ <strong>Goal</strong></p>
<div class="line-block">
<div class="line">We want to <strong>approximate the distribution of the data as a mixture of two Gaussians (K = 2).</strong></div>
<div class="line">Instead of saying ‚Äúthis point is in cluster 1‚Äù, we say this point <strong>probably</strong> came from <em>Gaussian1</em> with probability <span class="math notranslate nohighlight">\(\gamma\)</span>‚Ä¶</div>
</div>
<p>We‚Äôll use the <strong>Expectation-Maximization (EM)</strong> algorithm to learn the model.</p>
<p>The dataset:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Point</p></th>
<th class="head"><p>Coordinates</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>A</p></td>
<td><p>(1, 1)</p></td>
</tr>
<tr class="row-odd"><td><p>B</p></td>
<td><p>(2, 1)</p></td>
</tr>
<tr class="row-even"><td><p>C</p></td>
<td><p>(4, 3)</p></td>
</tr>
<tr class="row-odd"><td><p>D</p></td>
<td><p>(5, 4)</p></td>
</tr>
<tr class="row-even"><td><p>E</p></td>
<td><p>(3, 4)</p></td>
</tr>
</tbody>
</table>
<p>üìå <strong>Step 1: Initialization of parameters</strong></p>
<p><strong>We assume the data comes from a mixture of 2 Gaussian components (K = 2)</strong>. We initialize:</p>
<ul class="simple">
<li><p>Means <span class="math notranslate nohighlight">\((\mu)\)</span></p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\mu_1 = (1, 1), \mu_2 = (5, 4) \leftarrow\)</span> based on points A and D.</p>
<ul class="simple">
<li><p>Covariance <span class="math notranslate nohighlight">\((\Sigma)\)</span></p></li>
</ul>
<p>$:nbsphinx-math:<cite>Sigma</cite>_1 = <span class="math">\Sigma</span>_2 = $ identity matrix (simplified asumption)</p>
<ul class="simple">
<li><p>Mixing weight <span class="math notranslate nohighlight">\((\pi)\)</span> equal priors:</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\pi_1 = 0.5, \pi_2 = 0.5\)</span>.</p>
<p>üîÑ <strong>Step 2: E-step (Expectation)</strong></p>
<p>For each point <span class="math notranslate nohighlight">\(x_i\)</span>, compute the <strong>responsibility</strong> <span class="math notranslate nohighlight">\(\gamma_{i_k}\)</span>, which is the <strong>posterior probability</strong> that the point <span class="math notranslate nohighlight">\(i\)</span> belongs to cluster <span class="math notranslate nohighlight">\(k\)</span> given the current parameters <span class="math notranslate nohighlight">\(\pi_k, \mu_k, \Sigma_k\)</span>:</p>
<div class="math notranslate nohighlight">
\[\gamma_{ik} = P(z_i=k | x_i)\]</div>
<div class="math notranslate nohighlight">
\[\gamma_{ik} = \dfrac{\pi_k.N(x_i | \mu_k, \Sigma_k)}{\Sigma_{j=1}^K\pi_i.N(x_i | \mu_j, \Sigma_j)}\]</div>
<p>Where <span class="math notranslate nohighlight">\(N(x_i | \mu_k, \Sigma_k)\)</span> is the multivariate Gaussian likelihood of <span class="math notranslate nohighlight">\(x_i\)</span> under cluster <span class="math notranslate nohighlight">\(k\)</span> and given by <span class="math notranslate nohighlight">\(\dfrac{1}{(2\pi)^{d/2}|\Sigma_k|^{1/2}}.exp(-\dfrac{1}{2} (x_i - \mu_j)^T\Sigma_k^{-1}(x_i - \mu_j))\)</span> with <span class="math notranslate nohighlight">\(d\)</span> the dimensionality of <span class="math notranslate nohighlight">\(x_i\)</span>.</p>
<p>Example: compute for the Point <span class="math notranslate nohighlight">\(B:(2,1)\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\|\ B - \mu_1 \|\ ^2 = (2-1)^2 + (1-1)^2 = 1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\|\ B - \mu_2 \|\ ^2 = (2-5)^2 + (1-4)^2 = 9 + 9 = 18\)</span>
Then:
<span class="math notranslate nohighlight">\(\gamma_{B,1} = \dfrac{exp(-\dfrac{1}{2} \times 1)}{exp(-\dfrac{1}{2} \times 1) + exp(-\dfrac{1}{2} \times 18)} \approx \dfrac{0.607}{0.607 + 0.0001} \approx 0.607 &gt; 0.5\)</span></p></li>
</ul>
<p>So in the table: <span class="math notranslate nohighlight">\(B \rightarrow (\gamma_1 \approx 0.61, \gamma_2 \approx 0.39)\)</span></p>
<p>After computing responsibilities (approximate values):</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Point</p></th>
<th class="head"><p>Œ≥ (Cluster 1)</p></th>
<th class="head"><p>Œ≥ (Cluster 2)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>A</p></td>
<td><p>~1.00</p></td>
<td><p>~0.00</p></td>
</tr>
<tr class="row-odd"><td><p>B</p></td>
<td><p>~0.67</p></td>
<td><p>~0.39</p></td>
</tr>
<tr class="row-even"><td><p>C</p></td>
<td><p>~0.30</p></td>
<td><p>~0.70</p></td>
</tr>
<tr class="row-odd"><td><p>D</p></td>
<td><p>~0.00</p></td>
<td><p>~1.00</p></td>
</tr>
<tr class="row-even"><td><p>E</p></td>
<td><p>~0.10</p></td>
<td><p>~0.90</p></td>
</tr>
</tbody>
</table>
<p>(Simplified values based on proximity to cluster means).</p>
<div class="line-block">
<div class="line">This E-step gives <strong>soft assignment</strong> (probabilities).</div>
<div class="line">E-step uses the current parameters - it doesn‚Äôt improve them. It just tells you, given the current model, what the likely cluster memberships are. That‚Äôs not enough. Your goal is to fit the best model to the data - and that means improving the parameters (M-step).</div>
</div>
<p>üìà <strong>Step 3: M-step (Maximization)</strong>.</p>
<p>Now update the parameters based on the responsibilities (probabilities) of the E-step:</p>
<div class="line-block">
<div class="line"><strong>Context:</strong></div>
<div class="line">In soft clustering (as in GMM), each data point belongs partially to all clusters, with a degree of membership given by the responsibility <span class="math notranslate nohighlight">\(\gamma_{ik}\)</span>. So, instead of counting how many points are assigned to a cluster (as in K-means), we sum the partial responsibilities of each point for that cluster (<span class="math notranslate nohighlight">\(N_k = \sum_{i=1}^N\gamma_{ik}\)</span>).</div>
</div>
<ul class="simple">
<li><p>Updated means:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mu_k = \dfrac{\sum_{i=1}^N\gamma_{ik}.x_i}{\sum_{i=1}^N\gamma_{ik}} = \dfrac{\sum_{i=1}^N\gamma_{ik}.x_i}{N_k}\]</div>
<ul class="simple">
<li><p>Updated covariances:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\Sigma_k = \dfrac{\sum_{i=1}^N\gamma_{ik}.(x_i - \mu_k)(x_i - \mu_k)^T}{\sum_{i=1}^N\gamma_{ik}} = \dfrac{\sum_{i=1}^N\gamma_{ik}.(x_i - \mu_k)(x_i - \mu_k)^T}{N_k}\]</div>
<ul class="simple">
<li><p>Updated mixing weights (priors):</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\pi_k = \dfrac{1}{N}\sum_{i=1}^N\gamma_{ik} = \dfrac{N_k}{N}\]</div>
<p>Example: updating <span class="math notranslate nohighlight">\(\mu_1\)</span></p>
<p><span class="math notranslate nohighlight">\(\mu_1 = \dfrac{1 \times A + 0.67 \times B + 0.3 \times C + 0 \times D + 0.1 \times E}{1 + 0.67 + 0.3 + 0 + 0.1}\)</span></p>
<p>This new mean will shift slightly toward <span class="math notranslate nohighlight">\(B\)</span> and <span class="math notranslate nohighlight">\(C\)</span>.</p>
<p>üîÅ <strong>Step 4: Iterate E-step and M-step</strong></p>
<p>Repeat steps 2 and 3 until convergence (i.e., the parameters change become very little between iterations).</p>
<p>With each iteration:</p>
<ul class="simple">
<li><p>The responsibilities become sharper (closer to 0 or 1).</p></li>
<li><p>The means settle into two natural centers.</p></li>
<li><p>The weights <span class="math notranslate nohighlight">\(\pi_1\)</span> and <span class="math notranslate nohighlight">\(\pi_2\)</span> reflect the size of each cluster.</p></li>
</ul>
<p>‚úÖ <strong>Step 5: Final Cluster Assignment</strong></p>
<p>After convergence, we can:</p>
<ul class="simple">
<li><p>Keep the responsibilities for soft clustering, or</p></li>
<li><p>Assign each point to the cluster with the highest Œ≥ (hard clustering)</p></li>
</ul>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Point</p></th>
<th class="head"><p>Assigned Cluster (argmax Œ≥)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>A</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>B</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>C</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-odd"><td><p>D</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-even"><td><p>E</p></td>
<td><p>2</p></td>
</tr>
</tbody>
</table>
<p><span class="math notranslate nohighlight">\(\rightarrow\)</span> This matches the intuitive groupings: (A, B) vs. (C, D, E).</p>
<p>üìå <strong>Summary of GMM (EM Algorithm)</strong></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Step</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Init</p></td>
<td><p>Choose initial values for <span class="math notranslate nohighlight">\(\mu_k\)</span>, <span class="math notranslate nohighlight">\(\Sigma_k\)</span>, and <span class="math notranslate nohighlight">\(\pi_k\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>E-step</p></td>
<td><p>Compute responsibilities <span class="math notranslate nohighlight">\(\gamma_{ik}\)</span> for all data points</p></td>
</tr>
<tr class="row-even"><td><p>M-step</p></td>
<td><p>Update the parameters using the responsibilities</p></td>
</tr>
<tr class="row-odd"><td><p>Repeat</p></td>
<td><p>Until convergence of parameters</p></td>
</tr>
<tr class="row-even"><td><p>Output</p></td>
<td><p>Use soft or hard assignments to define clusters</p></td>
</tr>
</tbody>
</table>
<section id="Example-with-basic-Python-codes">
<h2>Example with basic Python codes<a class="headerlink" href="#Example-with-basic-Python-codes" title="Link to this heading">¬∂</a></h2>
<p>üìå <strong>Step 1: Initialization of parameters</strong></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[89]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
X = np.array([[1,1] , [2,1], [4,3], [5,4], [3,4]])
print(X)

# for reproducibility
np.random.seed(4)

# number of clusters
K = 2

#¬†randomly initialize the means by selecting K random points from the dataset
indices = np.random.choice(len(X), size=K, replace=False)
mu = X[indices].copy()  # shape (2, 4)
print(&quot;Initial means:&quot;)
mu1 = mu[:1]
print(f&quot;mu1 : {mu1}&quot;)
mu2 = mu[1:]
print(f&quot;mu2 : {mu2}&quot;)

# covariance matrices (identity matrices for simplicity)
print(&quot;Initial covariance matrix:&quot;)
cov1 = cov2 = np.eye(X.shape[1])
print(cov1)

pi1 = pi2 = .5  # equal priors for both clusters
print(&quot;Initial weights (priors):&quot;)
print(f&quot;pi1 : {pi1}&quot;)
print(f&quot;pi2 : {pi2}&quot;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[1 1]
 [2 1]
 [4 3]
 [5 4]
 [3 4]]
Initial means:
mu1 : [[1 1]]
mu2 : [[5 4]]
Initial covariance matrix:
[[1. 0.]
 [0. 1.]]
Initial weights (priors):
pi1 : 0.5
pi2 : 0.5
</pre></div></div>
</div>
<p>üîÑ <strong>Step 2: E-step (Expectation)</strong></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[90]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>#¬†E-step: calculate responsibilities
from scipy.stats import multivariate_normal

# Define the Gaussians
rv1 = multivariate_normal(mean=mu1.flatten(), cov=cov1)
rv2 = multivariate_normal(mean=mu2.flatten(), cov=cov2)

gammas = []  # list to store responsibilities

for x in X:
    # Compute likelihood under each Gaussian
    p1 = pi1 * rv1.pdf(x) # = joint proba to observe x and x is in the cluster 1 = prior √ó likelihood = responsibility numerator for cluster 1
    p2 = pi2 * rv2.pdf(x) # = joint proba to observe x and x is in the cluster 2 = prior √ó likelihood = responsibility numerator for cluster 2

    # Normalize to get gamma (posterior probability for cluster 1)
    gamma1 = p1 / (p1 + p2) # if gamma1 is above 0.5, then x is more likely to belong to cluster 1
    gamma2 = 1 - gamma1
    gammas.append([gamma1, gamma2])

# Convert to array for easier inspection
gammas = np.array(gammas)

# Show results
df = pd.DataFrame(gammas, columns=[&quot;Gamma (Cluster 1)&quot;, &quot;Gamma (Cluster 2)&quot;])
df.insert(0, &quot;Point&quot;, [f&quot;X{i}&quot; for i in range(1, len(X)+1)])
print(df)
#¬†Extra: two ways to calculate the probability of a point belonging to cluster 1 P(x_i | C_1))
# 1. Using the formula for multivariate Gaussian distribution
prob1 = (1 / np.sqrt((2 * np.pi) ** X.shape[1] * np.linalg.det(cov1))) * \
    np.exp(-0.5 * np.sum((X - mu1) @ np.linalg.inv(cov1) * (X - mu1), axis=1))
print(f&quot;Probability for cluster 1: {prob1}&quot;)

# 2. Using scipy&#39;s multivariate_normal for the same calculation
from scipy.stats import multivariate_normal
rv1 = multivariate_normal(mean=mu1.flatten(), cov=cov1)
prob1 = rv1.pdf(X)
print(f&quot;Probability for cluster 1: {prob1}&quot;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
  Point  Gamma (Cluster 1)  Gamma (Cluster 2)
0    X1           0.999996           0.000004
1    X2           0.999797           0.000203
2    X3           0.004070           0.995930
3    X4           0.000004           0.999996
4    X5           0.010987           0.989013
Probability for cluster 1: [1.59154943e-01 9.65323526e-02 2.39279779e-04 5.93115274e-07
 2.39279779e-04]
Probability for cluster 1: [1.59154943e-01 9.65323526e-02 2.39279779e-04 5.93115274e-07
 2.39279779e-04]
</pre></div></div>
</div>
<p>üîÅ <strong>Step 3: M-step (Maximization)</strong></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[91]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(&quot;\n--- M-step ---&quot;)
gamma1 = gammas[:, 0].reshape(-1, 1)  # Reshape for matrix operations
gamma2 = gammas[:, 1].reshape(-1, 1)
N1 = gamma1.sum()
N2 = gamma2.sum()

# Updated means
mu1_new = (gamma1 * X).sum(axis=0) / N1
mu2_new = (gamma2 * X).sum(axis=0) / N2

# Updated covariances
cov1_new = np.zeros((X.shape[1], X.shape[1]))
cov2_new = np.zeros((X.shape[1], X.shape[1]))
for i in range(len(X)):
    diff1 = (X[i] - mu1_new).reshape(-1, 1)
    diff2 = (X[i] - mu2_new).reshape(-1, 1)
    cov1_new += gamma1[i] * (diff1 @ diff1.T)
    cov2_new += gamma2[i] * (diff2 @ diff2.T)

cov1_new /= N1
cov2_new /= N2

# Updated weights. It&#39;s the MLE estimate for the priors.
pi1_new = N1 / len(X)
pi2_new = N2 / len(X)

# Show updated parameters
print(&quot;\n--- M-step ---&quot;)
print(&quot;Updated means:&quot;)
print(f&quot;mu1_new: {mu1_new}&quot;)
print(f&quot;mu2_new: {mu2_new}&quot;)
print(&quot;\nUpdated covariances:&quot;)
print(f&quot;cov1_new:\n{cov1_new}&quot;)
print(f&quot;cov2_new:\n{cov2_new}&quot;)
print(&quot;\nUpdated priors:&quot;)
print(f&quot;pi1_new: {pi1_new}&quot;)
print(f&quot;pi2_new: {pi2_new}&quot;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

--- M-step ---

--- M-step ---
Updated means:
mu1_new: [1.51318654 1.0204046 ]
mu2_new: [4.00353925 3.66616333]

Updated covariances:
cov1_new:
[[0.27287465 0.03438906]
 [0.03438906 0.05675732]]
cov2_new:
[[0.66657341 0.00160165]
 [0.00160165 0.22280612]]

Updated priors:
pi1_new: 0.4029707306736868
pi2_new: 0.5970292693263132
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[95]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal

# Dataset
X = np.array([[1, 1], [2, 1], [4, 3], [5, 4], [3, 4]])

# Initialization
np.random.seed(4)
K = 2
indices = np.random.choice(len(X), size=K, replace=False)
mu = X[indices].copy()
mu1 = mu[:1]
mu2 = mu[1:]
cov1 = cov2 = np.eye(X.shape[1])
pi1 = pi2 = 0.5

# EM loop
max_iter = 10
for iteration in range(max_iter):
    # E-step
    rv1 = multivariate_normal(mean=mu1.flatten(), cov=cov1, allow_singular=True)
    rv2 = multivariate_normal(mean=mu2.flatten(), cov=cov2, allow_singular=True)

    gammas = []
    for x in X:
        p1 = pi1 * rv1.pdf(x)
        p2 = pi2 * rv2.pdf(x)
        gamma1 = p1 / (p1 + p2)
        gamma2 = 1 - gamma1
        gammas.append([gamma1, gamma2])
    gammas = np.array(gammas)

    # M-step
    gamma1 = gammas[:, 0].reshape(-1, 1)
    gamma2 = gammas[:, 1].reshape(-1, 1)
    N1 = gamma1.sum()
    N2 = gamma2.sum()

    mu1_new = (gamma1 * X).sum(axis=0) / N1
    mu2_new = (gamma2 * X).sum(axis=0) / N2

    epsilon = 1e-6
    cov1_new += epsilon * np.eye(X.shape[1])
    cov2_new += epsilon * np.eye(X.shape[1])
    for i in range(len(X)):
        diff1 = (X[i] - mu1_new).reshape(-1, 1)
        diff2 = (X[i] - mu2_new).reshape(-1, 1)
        cov1_new += gamma1[i] * (diff1 @ diff1.T)
        cov2_new += gamma2[i] * (diff2 @ diff2.T)

    cov1_new /= N1
    cov2_new /= N2

    pi1_new = N1 / len(X)
    pi2_new = N2 / len(X)

    # Update parameters for next iteration
    mu1, mu2 = mu1_new, mu2_new
    cov1, cov2 = cov1_new, cov2_new
    pi1, pi2 = pi1_new, pi2_new

# -------------------------------
# Final results display
# -------------------------------

# Table of responsibilities
df = pd.DataFrame(gammas, columns=[&quot;Gamma (Cluster 1)&quot;, &quot;Gamma (Cluster 2)&quot;])
df.insert(0, &quot;Point&quot;, [f&quot;X{i}&quot; for i in range(1, len(X)+1)])
print(&quot;\nFinal responsibilities (posterior probabilities):&quot;)
print(df)

# Probability of a point belonging to cluster 1 ‚Äî manual method
prob1_manual = (1 / np.sqrt((2 * np.pi) ** X.shape[1] * np.linalg.det(cov1))) * \
    np.exp(-0.5 * np.sum((X - mu1) @ np.linalg.inv(cov1) * (X - mu1), axis=1))
print(&quot;\nProbability of belonging to cluster 1 (manual formula):&quot;)
print(prob1_manual)

# Probability of a point belonging to cluster 1 ‚Äî scipy method
rv1 = multivariate_normal(mean=mu1.flatten(), cov=cov1)
prob1_scipy = rv1.pdf(X)
print(&quot;\nProbability of belonging to cluster 1 (scipy):&quot;)
print(prob1_scipy)
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

Final responsibilities (posterior probabilities):
  Point  Gamma (Cluster 1)  Gamma (Cluster 2)
0    X1                1.0       9.111665e-09
1    X2                1.0       1.110065e-07
2    X3                0.0       1.000000e+00
3    X4                0.0       1.000000e+00
4    X5                0.0       1.000000e+00

Probability of belonging to cluster 1 (manual formula):
[16.57636077 16.57635993  0.          0.          0.        ]

Probability of belonging to cluster 1 (scipy):
[16.57636077 16.57635993  0.          0.          0.        ]
</pre></div></div>
</div>
</section>
<section id="Example-with-sklearn-on-the-Iris-dataset">
<h2>Example with sklearn on the Iris dataset<a class="headerlink" href="#Example-with-sklearn-on-the-Iris-dataset" title="Link to this heading">¬∂</a></h2>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[92]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from sklearn import datasets
from sklearn.mixture import GaussianMixture
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt

# Load the iris dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target
species = iris.target_names

# Fit Gaussian Mixture Models with 1 to 10 components and compute BIC
bics = [] #¬†Bayesian Information Criterion, BIC = -2log(L) + k log(n), where k is the number of parameters, n is the number of data points and L is the likelihood of the model given the data.
models = []
for n in range(1, 11):
    gmm = GaussianMixture(n_components=n, covariance_type=&#39;full&#39;, random_state=0)
    gmm.fit(X)
    bics.append(gmm.bic(X))
    models.append(gmm)

# Plot BIC values
plt.plot(range(1, 11), bics, marker=&#39;o&#39;)
plt.xlabel(&#39;Number of components&#39;)
plt.ylabel(&#39;BIC&#39;)
plt.title(&#39;BIC for GMM on Iris dataset&#39;)
plt.show()

# According to BIC, the best model has 2 components
best_n = np.argmin(bics) + 1
print(f&quot;Best number of components according to BIC: {best_n}&quot;)

# Fit GMM with 2 components
gmm_best = GaussianMixture(n_components=best_n, covariance_type=&#39;full&#39;, random_state=0)
gmm_best.fit(X)
labels = gmm_best.predict(X)

# Contingency table between predicted classes and true species
pd.crosstab(labels, y, rownames=[&#39;Predicted&#39;], colnames=[&#39;True Species&#39;])

# If we force a model with 3 components (as in the R code)
gmm_3 = GaussianMixture(n_components=3, covariance_type=&#39;full&#39;, random_state=0)
gmm_3.fit(X)
labels_3 = gmm_3.predict(X)

# Contingency table for 3 components
pd.crosstab(labels_3, y, rownames=[&#39;Predicted (3 classes)&#39;], colnames=[&#39;True Species&#39;])
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_4_GMM_15_0.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_4_GMM_15_0.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Best number of components according to BIC: 2
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[92]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>True Species</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
    </tr>
    <tr>
      <th>Predicted (3 classes)</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>45</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>50</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>5</td>
      <td>50</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p><strong>END</strong></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="ML_labDauphine_unsupervised_3_DBSCAN.html" class="btn btn-neutral float-left" title="Clustering: DBSCAN" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ML_labDauphine_unsupervised_5_ISOMAP.html" class="btn btn-neutral float-right" title="Clustering: ISOMAP" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, David TBO.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>