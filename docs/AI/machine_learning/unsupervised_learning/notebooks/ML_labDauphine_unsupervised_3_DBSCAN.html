

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Clustering: DBSCAN &mdash; Machine Learning and Deep Learning 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/nbsphinx-code-cells.css?v=2aa19091" />

  
      <script src="../../../../_static/documentation_options.js?v=2709fde1"></script>
      <script src="../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Clustering: Gaussian Mixed Mixture (GMM)" href="ML_labDauphine_unsupervised_4_GMM.html" />
    <link rel="prev" title="Clustering: KMeans" href="ML_labDauphine_unsupervised_2_Kmeans.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            Machine Learning and Deep Learning
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../index.html">Machine Learning</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../index.html#notebooks">Notebooks</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../index.html">Unsupervised learning</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="../index.html#notebooks">Notebooks</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">Machine Learning and Deep Learning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Machine Learning</a></li>
          <li class="breadcrumb-item"><a href="../index.html">Unsupervised learning</a></li>
      <li class="breadcrumb-item active">Clustering: DBSCAN</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../_sources/AI/machine_learning/unsupervised_learning/notebooks/ML_labDauphine_unsupervised_3_DBSCAN.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Clustering:-DBSCAN">
<h1>Clustering: DBSCAN<a class="headerlink" href="#Clustering:-DBSCAN" title="Link to this heading">¶</a></h1>
<div class="line-block">
<div class="line"><strong>DBSCAN</strong> (<strong>D</strong>ensity-<strong>B</strong>ased <strong>S</strong>patial <strong>C</strong>lustering of <strong>A</strong>pplications with <strong>N</strong>oise)</div>
<div class="line">groups points that are closely packed together (i.e., with many nearby neighbors),</div>
<div class="line">and marks points that lie alone in low-density regions as noise.</div>
</div>
<p>It uses two parameters:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> (epsilon): radius of the neighborhood around a point.</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{minPts}\)</span>: minimum number of points (including the point itself) required to form a dense region. <strong>Dataset</strong>:</p></li>
</ul>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Point</p></th>
<th class="head"><p>Coordinates</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>A</p></td>
<td><p>(1, 1)</p></td>
</tr>
<tr class="row-odd"><td><p>B</p></td>
<td><p>(2, 1)</p></td>
</tr>
<tr class="row-even"><td><p>C</p></td>
<td><p>(4, 3)</p></td>
</tr>
<tr class="row-odd"><td><p>D</p></td>
<td><p>(5, 4)</p></td>
</tr>
<tr class="row-even"><td><p>E</p></td>
<td><p>(3, 4)</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p><strong>Step 0: Choose Parameters</strong></p></li>
</ul>
<p>Let’s choose:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\epsilon = 1.5\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(minPts = 2\)</span> (to keep it simple for this small dataset).</p></li>
<li><p><strong>Step 1: Compute Distance Matrix</strong></p></li>
</ul>
<p>We compute Euclidean distances (already computed in your HAC example):</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>A</p></th>
<th class="head"><p>B</p></th>
<th class="head"><p>C</p></th>
<th class="head"><p>D</p></th>
<th class="head"><p>E</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>A</p></td>
<td><p>0.0</p></td>
<td><p>1.0</p></td>
<td><p>3.6</p></td>
<td><p>5.0</p></td>
<td><p>3.6</p></td>
</tr>
<tr class="row-odd"><td><p>B</p></td>
<td><p>1.0</p></td>
<td><p>0.0</p></td>
<td><p>2.8</p></td>
<td><p>4.2</p></td>
<td><p>3.2</p></td>
</tr>
<tr class="row-even"><td><p>C</p></td>
<td><p>3.6</p></td>
<td><p>2.8</p></td>
<td><p>0.0</p></td>
<td><p>1.4</p></td>
<td><p>1.4</p></td>
</tr>
<tr class="row-odd"><td><p>D</p></td>
<td><p>5.0</p></td>
<td><p>4.2</p></td>
<td><p>1.4</p></td>
<td><p>0.0</p></td>
<td><p>2.0</p></td>
</tr>
<tr class="row-even"><td><p>E</p></td>
<td><p>3.6</p></td>
<td><p>3.2</p></td>
<td><p>1.4</p></td>
<td><p>2.0</p></td>
<td><p>0.0</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p><strong>Step 2: Identify Core, Border, and Noise Points</strong></p></li>
</ul>
<div class="line-block">
<div class="line">A: Neighbors within <span class="math notranslate nohighlight">\(\epsilon\)</span> = [B] <span class="math notranslate nohighlight">\(\rightarrow\)</span> only 1 neighbor <span class="math notranslate nohighlight">\(\rightarrow\)</span> not core</div>
<div class="line">B: Neighbors within <span class="math notranslate nohighlight">\(\epsilon\)</span> = [A] <span class="math notranslate nohighlight">\(\rightarrow\)</span> only 1 neighbor <span class="math notranslate nohighlight">\(\rightarrow\)</span> not core</div>
<div class="line">C: Neighbors within <span class="math notranslate nohighlight">\(\epsilon\)</span> = [D, E] <span class="math notranslate nohighlight">\(\rightarrow\)</span> 2 neighbors <span class="math notranslate nohighlight">\(\rightarrow\)</span> ✅ core</div>
<div class="line">D: Neighbors within <span class="math notranslate nohighlight">\(\epsilon\)</span> = [C] <span class="math notranslate nohighlight">\(\rightarrow\)</span> only 1 <span class="math notranslate nohighlight">\(\rightarrow\)</span> not core</div>
<div class="line">E: Neighbors within <span class="math notranslate nohighlight">\(\epsilon\)</span> = [C] <span class="math notranslate nohighlight">\(\rightarrow\)</span> only 1 <span class="math notranslate nohighlight">\(\rightarrow\)</span> not core</div>
</div>
<p>So:</p>
<div class="line-block">
<div class="line">Core points: C</div>
<div class="line">Border points: D, E (because they’re close to a core point)</div>
<div class="line">Noise: A, B (not core, and not close to a core).</div>
</div>
<ul class="simple">
<li><p><strong>Step 3: Form Clusters</strong></p></li>
</ul>
<div class="line-block">
<div class="line">Start with core point C → new cluster Cluster 1</div>
<div class="line">Add C’s ε-neighbors → D and E</div>
<div class="line"><span class="math notranslate nohighlight">\(\rightarrow\)</span> D and E are added to Cluster 1 as border points</div>
</div>
<p>So:</p>
<div class="line-block">
<div class="line">Cluster 1 = {C, D, E}</div>
<div class="line">A and B remain unclustered (noise)</div>
</div>
<p><strong>Final Result</strong>:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Cluster</p></th>
<th class="head"><p>Points</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>C, D, E</p></td>
</tr>
<tr class="row-odd"><td><p>Noise</p></td>
<td><p>A, B</p></td>
</tr>
</tbody>
</table>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
from sklearn.cluster import DBSCAN

# Step-by-step DBSCAN explanation with a small 2D dataset

import matplotlib.pyplot as plt

# 1. Define the dataset
# Points: A(1,1), B(2,1), C(4,3), D(5,4), E(3,4)
points = np.array([
    [1, 1],  # A
    [2, 1],  # B
    [4, 3],  # C
    [5, 4],  # D
    [3, 4],  # E
])
labels_names = [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;]

# 2. Visualize the points
plt.figure(figsize=(5, 5))
plt.scatter(points[:, 0], points[:, 1], color=&#39;blue&#39;)
for i, txt in enumerate(labels_names):
    plt.annotate(txt, (points[i, 0]+0.05, points[i, 1]+0.05), fontsize=12)
plt.title(&quot;Step 1: Visualize the data points&quot;)
plt.xlabel(&quot;X&quot;)
plt.ylabel(&quot;Y&quot;)
plt.grid(True)
plt.show()

# 3. Choose DBSCAN parameters
# Let&#39;s set eps=1.5 (radius), min_samples=2 (minimum points to form a cluster)
eps = 1.5
min_samples = 2

# 4. Visualize the eps-neighborhood for point A
plt.figure(figsize=(5, 5))
plt.scatter(points[:, 0], points[:, 1], color=&#39;blue&#39;)
plt.scatter(points[0, 0], points[0, 1], color=&#39;red&#39;, label=&#39;Point A&#39;)
circle = plt.Circle((points[0, 0], points[0, 1]), eps, color=&#39;red&#39;, fill=False, linestyle=&#39;--&#39;, label=&#39;eps-neighborhood&#39;)
plt.gca().add_patch(circle)
for i, txt in enumerate(labels_names):
    plt.annotate(txt, (points[i, 0]+0.05, points[i, 1]+0.05), fontsize=12)
plt.title(&quot;Step 2: eps-neighborhood of point A&quot;)
plt.xlabel(&quot;X&quot;)
plt.ylabel(&quot;Y&quot;)
plt.legend()
plt.grid(True)
plt.show()

# 5. Run DBSCAN
db = DBSCAN(eps=eps, min_samples=min_samples)
db_labels = db.fit_predict(points)

# 6. Visualize the clustering result
colors = [&#39;tab:blue&#39;, &#39;tab:orange&#39;, &#39;tab:green&#39;, &#39;tab:red&#39;, &#39;tab:purple&#39;]
plt.figure(figsize=(5, 5))
for i in range(len(points)):
    if db_labels[i] == -1:
        plt.scatter(points[i, 0], points[i, 1], color=&#39;k&#39;, marker=&#39;x&#39;, s=100, label=&#39;Noise&#39; if i == 0 else &quot;&quot;)
    else:
        plt.scatter(points[i, 0], points[i, 1], color=colors[db_labels[i]], s=100, label=f&#39;Cluster {db_labels[i]+1}&#39; if f&#39;Cluster {db_labels[i]+1}&#39; not in plt.gca().get_legend_handles_labels()[1] else &quot;&quot;)
    plt.annotate(labels_names[i], (points[i, 0]+0.05, points[i, 1]+0.05), fontsize=12)
plt.title(&quot;Step 3: DBSCAN clustering result&quot;)
plt.xlabel(&quot;X&quot;)
plt.ylabel(&quot;Y&quot;)
plt.legend()
plt.grid(True)
plt.show()

# 7. Print cluster assignments
for i, name in enumerate(labels_names):
    label = db_labels[i]
    if label == -1:
        print(f&quot;Point {name} is classified as Noise.&quot;)
    else:
        print(f&quot;Point {name} is assigned to Cluster {label+1}.&quot;)

# --- Explanations (in English) ---
# Step 1: We plot the five points in 2D space.
# Step 2: We illustrate the eps-neighborhood (radius=1.5) for point A. Any point within this circle is considered a neighbor.
# Step 3: We run DBSCAN. Points with at least &#39;min_samples&#39; (here, 2) neighbors (including itself) within eps are core points and can form clusters.
#         Points not belonging to any cluster are labeled as noise (black &#39;x&#39;).
#         The plot shows the resulting clusters and noise points.
# The printed output shows the cluster assignment for each point.
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_3_DBSCAN_3_0.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_3_DBSCAN_3_0.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_3_DBSCAN_3_1.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_3_DBSCAN_3_1.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_3_DBSCAN_3_2.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_3_DBSCAN_3_2.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Point A is assigned to Cluster 1.
Point B is assigned to Cluster 1.
Point C is assigned to Cluster 2.
Point D is assigned to Cluster 2.
Point E is assigned to Cluster 2.
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from sklearn.cluster import DBSCAN
import numpy as np
import pandas as pd
from scipy.spatial.distance import cdist

# DBSCAN explanation step-by-step with a small dataset

# Dataset : 5 points in 2D
# | Point | Coordinates |
# | ----- | ----------- |
# | A     | (1, 1)      |
# | B     | (2, 1)      |
# | C     | (4, 3)      |
# | D     | (5, 4)      |
# | E     | (3, 4)      |


# 1. Dataset preparation
points = np.array([
    [1, 1],  # A
    [2, 1],  # B
    [4, 3],  # C
    [5, 4],  # D
    [3, 4],  # E
])
labels_names = [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;]

# 2. Choix des paramètres DBSCAN
eps = 1.5  # rayon de voisinage
min_samples = 2  # nombre minimum de points pour former un cluster

# 3. Calcul des voisins pour chaque point

dist_matrix = pd.DataFrame(
    cdist(points, points),
    columns=labels_names,
    index=labels_names
)
print(&quot;Matrice des distances euclidiennes :&quot;)
print(dist_matrix.round(2))

# 4. Application de DBSCAN
db = DBSCAN(eps=eps, min_samples=min_samples)
db_labels = db.fit_predict(points)

# 5. Explication étape par étape (en français)
print(&quot;\nÉtape 1 : On considère chaque point individuellement.&quot;)
for i, name in enumerate(labels_names):
    print(f&quot;Point {name} : {points[i]}&quot;)

print(f&quot;\nÉtape 2 : Pour chaque point, on regarde les voisins dans un rayon eps={eps}.&quot;)
for i, name in enumerate(labels_names):
    neighbors = [labels_names[j] for j in range(len(points)) if dist_matrix.iloc[i, j] &lt;= eps and i != j]
    print(f&quot;Point {name} a pour voisins (dans eps) : {neighbors}&quot;)

print(f&quot;\nÉtape 3 : On identifie les points centraux (core points) :&quot;)
for i, name in enumerate(labels_names):
    neighbors_count = sum(dist_matrix.iloc[i, :] &lt;= eps)
    if neighbors_count &gt;= min_samples:
        print(f&quot;Point {name} est un point central (core point) ({neighbors_count} dans eps).&quot;)
    else:
        print(f&quot;Point {name} n&#39;est PAS un point central ({neighbors_count} dans eps).&quot;)

print(&quot;\nÉtape 4 : Construction des clusters par propagation :&quot;)
for i, name in enumerate(labels_names):
    label = db_labels[i]
    if label == -1:
        print(f&quot;Point {name} est du bruit (noise).&quot;)
    else:
        print(f&quot;Point {name} appartient au cluster {label+1}.&quot;)

# Résumé des clusters
clusters = {}
for i, label in enumerate(db_labels):
    if label == -1:
        continue
    clusters.setdefault(label+1, []).append(labels_names[i])
print(&quot;\nRésumé des clusters trouvés :&quot;)
for k, v in clusters.items():
    print(f&quot;Cluster {k} : {v}&quot;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Matrice des distances euclidiennes :
      A     B     C     D     E
A  0.00  1.00  3.61  5.00  3.61
B  1.00  0.00  2.83  4.24  3.16
C  3.61  2.83  0.00  1.41  1.41
D  5.00  4.24  1.41  0.00  2.00
E  3.61  3.16  1.41  2.00  0.00

Étape 1 : On considère chaque point individuellement.
Point A : [1 1]
Point B : [2 1]
Point C : [4 3]
Point D : [5 4]
Point E : [3 4]

Étape 2 : Pour chaque point, on regarde les voisins dans un rayon eps=1.5.
Point A a pour voisins (dans eps) : [&#39;B&#39;]
Point B a pour voisins (dans eps) : [&#39;A&#39;]
Point C a pour voisins (dans eps) : [&#39;D&#39;, &#39;E&#39;]
Point D a pour voisins (dans eps) : [&#39;C&#39;]
Point E a pour voisins (dans eps) : [&#39;C&#39;]

Étape 3 : On identifie les points centraux (core points) :
Point A est un point central (core point) (2 dans eps).
Point B est un point central (core point) (2 dans eps).
Point C est un point central (core point) (3 dans eps).
Point D est un point central (core point) (2 dans eps).
Point E est un point central (core point) (2 dans eps).

Étape 4 : Construction des clusters par propagation :
Point A appartient au cluster 1.
Point B appartient au cluster 1.
Point C appartient au cluster 2.
Point D appartient au cluster 2.
Point E appartient au cluster 2.

Résumé des clusters trouvés :
Cluster 1 : [&#39;A&#39;, &#39;B&#39;]
Cluster 2 : [&#39;C&#39;, &#39;D&#39;, &#39;E&#39;]
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.metrics import adjusted_rand_score
from sklearn.neighbors import NearestNeighbors

# Import necessary libraries
import matplotlib.pyplot as plt

# cluster.colors equivalent: using matplotlib colormaps
import matplotlib.cm as cm

# --- DBSCAN ---

# --- A first simulated example ---
np.random.seed(2)
n = 400

# Generate 4 clusters with some noise
# In R: runif(4, 0, 1) + rnorm(n, sd = 0.1)
# In Python: np.random.uniform + np.random.normal
x_centers = np.random.uniform(0, 1, 4)
y_centers = np.random.uniform(0, 1, 4)
x = np.vstack([
    np.column_stack((
        np.full(100, x_centers[i]) + np.random.normal(0, 0.1, 100),
        np.full(100, y_centers[i]) + np.random.normal(0, 0.1, 100)
    )) for i in range(4)
])
true_clusters = np.repeat(np.arange(1, 5), 100)

# Plot the data colored by true clusters
plt.figure(figsize=(6, 5))
plt.scatter(x[:, 0], x[:, 1], c=true_clusters, cmap=&#39;Set2&#39;, marker=&#39;o&#39;)
plt.title(&quot;Simulated data with true clusters&quot;)
plt.show()

# --- How to choose hyperparameters? ---
# Heuristics:
# - Choose minPts = 2p (current recommendations) - or more if data is noisy
# - Use a k-NN approach to choose epsilon

# For the example above, minPts = 4
# Plot the k-NN distances to help choose epsilon
neighbors = NearestNeighbors(n_neighbors=4)
neighbors_fit = neighbors.fit(x)
distances, indices = neighbors_fit.kneighbors(x)
distances = np.sort(distances[:, 3])  # 4th neighbor distance

plt.figure(figsize=(6, 4))
plt.plot(distances)
plt.ylabel(&quot;4-NN distance&quot;)
plt.xlabel(&quot;Points sorted by distance&quot;)
plt.title(&quot;k-NN distance plot (k=4)&quot;)
plt.show()
# From the plot, values around 0.05 seem suitable

# Run DBSCAN with eps=0.05, minPts=4
db = DBSCAN(eps=0.05, min_samples=4).fit(x)
labels = db.labels_

plt.figure(figsize=(6, 5))
# Noise is labeled as -1 in sklearn, so add 1 for color mapping
plt.scatter(x[:, 0], x[:, 1], c=labels + 1, cmap=&#39;Set2&#39;, marker=&#39;o&#39;)
plt.title(&quot;DBSCAN clustering (eps=0.05, minPts=4)&quot;)
plt.show()
# Noise is shown in black

# Note: The k-NN approach for choosing epsilon can be slow if n is large
# An alternative is to use a grid search
# The Adjusted Rand Index (ARI) is used to compare results to true classes
# (This is for illustration only; in real data, the partition is unknown)

minPts_grid = np.arange(1, 21)
eps_grid = np.arange(0.01, 0.21, 0.01)
res_mat = np.zeros((len(minPts_grid), len(eps_grid)))

for i, minPts in enumerate(minPts_grid):
    for j, eps in enumerate(eps_grid):
        db = DBSCAN(eps=eps, min_samples=minPts).fit(x)
        res_mat[i, j] = adjusted_rand_score(true_clusters, db.labels_)

plt.figure(figsize=(8, 6))
plt.imshow(res_mat, aspect=&#39;auto&#39;, origin=&#39;lower&#39;,
           extent=[eps_grid[0], eps_grid[-1], minPts_grid[0], minPts_grid[-1]],
           cmap=&#39;viridis&#39;)
plt.colorbar(label=&#39;ARI&#39;)
plt.xlabel(&#39;eps&#39;)
plt.ylabel(&#39;minPts&#39;)
plt.title(&#39;ARI for different DBSCAN parameters&#39;)
plt.show()

# Example with eps=0.09, minPts=15
db = DBSCAN(eps=0.09, min_samples=15).fit(x)
labels = db.labels_

plt.figure(figsize=(6, 5))
plt.scatter(x[:, 0], x[:, 1], c=labels + 1, cmap=&#39;Set2&#39;, marker=&#39;o&#39;)
plt.title(&quot;DBSCAN clustering (eps=0.09, minPts=15)&quot;)
plt.show()

# --- Explanations ---
# - The code simulates 4 clusters and applies DBSCAN clustering.
# - The k-NN distance plot helps to select a suitable epsilon value.
# - Grid search with ARI helps to compare clustering results to the true labels.
# - The final plots show the clustering results for different parameter choices.
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_3_DBSCAN_5_0.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_3_DBSCAN_5_0.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_3_DBSCAN_5_1.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_3_DBSCAN_5_1.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_3_DBSCAN_5_2.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_3_DBSCAN_5_2.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/Users/davidtbo/Library/Mobile Documents/com~apple~CloudDocs/AI/machine_learning/unsupervised_learning/.venv/lib/python3.13/site-packages/sklearn/metrics/cluster/_supervised.py:50: UserWarning: The number of unique classes is greater than 50% of the number of samples.
  type_pred = type_of_target(labels_pred)
/Users/davidtbo/Library/Mobile Documents/com~apple~CloudDocs/AI/machine_learning/unsupervised_learning/.venv/lib/python3.13/site-packages/sklearn/metrics/cluster/_supervised.py:50: UserWarning: The number of unique classes is greater than 50% of the number of samples.
  type_pred = type_of_target(labels_pred)
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_3_DBSCAN_5_4.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_3_DBSCAN_5_4.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_3_DBSCAN_5_5.png" src="../../../../_images/AI_machine_learning_unsupervised_learning_notebooks_ML_labDauphine_unsupervised_3_DBSCAN_5_5.png" />
</div>
</div>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="ML_labDauphine_unsupervised_2_Kmeans.html" class="btn btn-neutral float-left" title="Clustering: KMeans" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ML_labDauphine_unsupervised_4_GMM.html" class="btn btn-neutral float-right" title="Clustering: Gaussian Mixed Mixture (GMM)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, David TBO.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>